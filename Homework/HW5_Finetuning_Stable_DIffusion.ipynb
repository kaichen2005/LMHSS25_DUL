{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b53b4dac",
   "metadata": {},
   "source": [
    "# Text-to-Image Generation using Stable Diffusion - Homework Assignment\n",
    "\n",
    "![Stable Diffusion Architecture](https://miro.medium.com/v2/resize:fit:1400/1*NpQ282NJdOfxUsYlwLJplA.png)\n",
    "\n",
    "In this homework, you will finetune a **Stable Diffusion** model to generate Naruto-style images from text descriptions. This involves working with the complete diffusion pipeline including VAE, UNet, text encoder, and scheduler.\n",
    "\n",
    "## ğŸ“Œ Project Overview\n",
    "- **Task**: Text-to-Naruto image generation\n",
    "- **Architecture**: Stable Diffusion with UNet diffusion model\n",
    "- **Dataset**: Naruto-style dataset with text descriptions\n",
    "- **Goal**: Generate realistic Naruto-style images from text prompts\n",
    "\n",
    "## ğŸ“š Learning Objectives\n",
    "By completing this assignment, you will:\n",
    "- Understand diffusion models and the stable diffusion pipeline\n",
    "- Learn to finetune pre-trained diffusion models\n",
    "- Work with VAE, UNet, text encoders, and schedulers\n",
    "- Practice text-to-image generation techniques\n",
    "- Handle memory constraints with large models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f022d71",
   "metadata": {},
   "source": [
    "## 1ï¸âƒ£ Dataset Setup (PROVIDED)\n",
    "\n",
    "The Naruto-style dataset has been loaded for you. The dataset contains:\n",
    "- 1,221 training images with corresponding text descriptions\n",
    "- Each sample has an 'image' and 'text' field\n",
    "- Images are in various sizes and need to be resized to 512x512\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a58ad0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12508bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U datasets\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3eeecc38",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-12T00:57:23.121088Z",
     "start_time": "2025-07-12T00:57:22.525983Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset info: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['image', 'text'],\n",
      "        num_rows: 1221\n",
      "    })\n",
      "})\n",
      "Number of training samples: 1221\n",
      "\n",
      "Sample text: a man with dark hair and brown eyes, naruto style\n",
      "Image size: (1080, 1080)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load dataset without any cache directory specified\n",
    "ds = load_dataset(\"Alex-0402/naruto-style-dataset-with-text\")\n",
    "print(\"Dataset info:\", ds)\n",
    "print(\"Number of training samples:\", len(ds['train']))\n",
    "\n",
    "# Display a sample\n",
    "sample = ds['train'][0]\n",
    "print(\"\\nSample text:\", sample['text'])\n",
    "print(\"Image size:\", sample['image'].size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14dd0a00",
   "metadata": {},
   "source": [
    "## 2ï¸âƒ£ Import Libraries and Configuration\n",
    "\n",
    "**Task**: Import all necessary libraries and set up configuration parameters.\n",
    "\n",
    "**Requirements**:\n",
    "- Import diffusers, transformers, and related libraries\n",
    "- Import PyTorch, PIL, numpy, and other utilities\n",
    "- Set random seeds for reproducibility\n",
    "- Configure hyperparameters for stable diffusion training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74428cb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPS device for Apple Silicon GPU acceleration.\n",
      "device now using: mps\n"
     ]
    }
   ],
   "source": [
    "# TODO: Import all necessary libraries:\n",
    "#       - torch, torch.nn.functional, torch.optim\n",
    "#       - diffusers (UNet2DConditionModel, AutoencoderKL, PNDMScheduler, DDPMScheduler)\n",
    "#       - transformers (CLIPTextModel, CLIPTokenizer)\n",
    "#       - PIL, numpy, matplotlib\n",
    "#       - torchvision.transforms\n",
    "#       - tqdm for progress bars\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from diffusers import UNet2DConditionModel, AutoencoderKL, PNDMScheduler, DDPMScheduler\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "from tqdm.auto import tqdm\n",
    "import random\n",
    "\n",
    "# TODO: Set random seeds for reproducibility (use seed=42)\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# TODO: Check device availability and print\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"device now using: {device}\")\n",
    "\n",
    "# TODO: Define configuration parameters:\n",
    "MODEL_ID = \"OFA-Sys/small-stable-diffusion-v0\"  # Smaller stable diffusion model\n",
    "IMG_SIZE = 512  # Image resolution\n",
    "BATCH_SIZE = 1  # Small batch size for memory constraints\n",
    "LEARNING_RATE = 1e-5  # Learning rate for finetuning\n",
    "NUM_EPOCHS = 6  # Number of training epochs\n",
    "INFERENCE_STEPS = 200  # Number of denoising steps during inference\n",
    "GUIDANCE_SCALE = 7.5  # Classifier-free guidance scale"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b488affe",
   "metadata": {},
   "source": [
    "è®¡ç®—æœºç”Ÿæˆçš„â€œéšæœºâ€å…¶å®æ˜¯â€œ**ä¼ªéšæœº**â€ **(Pseudo-random)**ã€‚\n",
    "\n",
    "ç®€å•æ¥è¯´ï¼šéšæœºç§å­æ˜¯ä¼ªéšæœºæ•°ç”Ÿæˆç®—æ³•çš„èµ·å§‹ç‚¹ã€‚åªè¦èµ·å§‹ç‚¹ç›¸åŒï¼Œç”Ÿæˆçš„â€œéšæœºâ€æ•°å­—åºåˆ—å°±å°†å®Œå…¨ä¸€æ ·ã€‚\n",
    "\n",
    "æ‰€ä»¥ï¼Œè®¾ç½®éšæœºç§å­æœ‰åˆ©äºæ¨¡å‹å¤ç°ï¼"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2ff29e",
   "metadata": {},
   "source": [
    "## 3ï¸âƒ£ Load Pre-trained Stable Diffusion Components\n",
    "\n",
    "**Task**: Load all components of the stable diffusion pipeline.\n",
    "\n",
    "**Requirements**:\n",
    "- Load VAE (Variational Autoencoder) for image encoding/decoding\n",
    "- Load UNet for the diffusion process\n",
    "- Load text encoder and tokenizer for text conditioning\n",
    "- Load noise scheduler for the diffusion process\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d2312cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error occurred while trying to fetch OFA-Sys/small-stable-diffusion-v0: OFA-Sys/small-stable-diffusion-v0 does not appear to have a file named diffusion_pytorch_model.safetensors.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n",
      "An error occurred while trying to fetch OFA-Sys/small-stable-diffusion-v0: OFA-Sys/small-stable-diffusion-v0 does not appear to have a file named diffusion_pytorch_model.safetensors.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n",
      "The config attributes {'predict_epsilon': True} were passed to PNDMScheduler, but are not expected and will be ignored. Please verify your scheduler_config.json configuration file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNet has 579,384,964 trainable parameters.\n",
      "VAE has 83,653,863 trainable parameters (frozen).\n",
      "Text Encoder has 123,060,480 trainable parameters (frozen).\n"
     ]
    }
   ],
   "source": [
    "# TODO: Load stable diffusion components:\n",
    "#       - vae = AutoencoderKL.from_pretrained(MODEL_ID, subfolder=\"vae\")\n",
    "#       - unet = UNet2DConditionModel.from_pretrained(MODEL_ID, subfolder=\"unet\")\n",
    "#       - text_encoder = CLIPTextModel.from_pretrained(MODEL_ID, subfolder=\"text_encoder\")\n",
    "#       - tokenizer = CLIPTokenizer.from_pretrained(MODEL_ID, subfolder=\"tokenizer\")\n",
    "#       - scheduler = PNDMScheduler.from_pretrained(MODEL_ID, subfolder=\"scheduler\")\n",
    "\n",
    "vae = AutoencoderKL.from_pretrained(MODEL_ID, subfolder=\"vae\")\n",
    "unet = UNet2DConditionModel.from_pretrained(MODEL_ID, subfolder=\"unet\")\n",
    "text_encoder = CLIPTextModel.from_pretrained(MODEL_ID, subfolder=\"text_encoder\")\n",
    "tokenizer = CLIPTokenizer.from_pretrained(MODEL_ID, subfolder=\"tokenizer\")\n",
    "scheduler = PNDMScheduler.from_pretrained(MODEL_ID, subfolder=\"scheduler\") \n",
    "# DDPMScheduler is a foundational scheduler used primarily for training\n",
    "# while PNDMScheduler is a more advanced scheduler designed for fast and high-quality inference (image generation).\n",
    "\n",
    "# TODO: Move models to device\n",
    "vae.to(device)\n",
    "unet.to(device)\n",
    "text_encoder.to(device)\n",
    "\n",
    "# TODO: Set VAE and text encoder to eval mode (only UNet will be trained)\n",
    "vae.eval()\n",
    "text_encoder.eval()\n",
    "\n",
    "# TODO: Print model information and parameter counts\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    # numel() is a method available on tensor objects (e.g., PyTorch tensors) that returns the total number of elements (or scalar values) in that tensor.\n",
    "\n",
    "print(f\"UNet has {count_parameters(unet):,} trainable parameters.\")\n",
    "print(f\"VAE has {count_parameters(vae):,} trainable parameters (frozen).\")\n",
    "print(f\"Text Encoder has {count_parameters(text_encoder):,} trainable parameters (frozen).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50328c67",
   "metadata": {},
   "source": [
    "The `scheduler` is responsible for **defining the diffusion process and how noise is added and removed at each step during the sampling (inference) process**.\n",
    "\n",
    "More specifically, the `scheduler` handles:\n",
    "\n",
    "1. **Noise Scheduling**: It determines the amount of noise to add or remove at each step of the denoising process.\n",
    "\n",
    "2. **Denoising Algorithm**: It implements the specific algorithm used to denoise the latent representation.\n",
    "\n",
    "3. **Timesteps**: It manages the timesteps (or noise levels) through which the model progressively denoises the latent representation.\n",
    "\n",
    "4. **Prediction Type**: It often handles whether the model is predicting the noise itself, the original image, or the velocity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc4f126",
   "metadata": {},
   "source": [
    "`if p.requires_grad`:\n",
    "* This is a crucial **filter** within the generator expression.\n",
    "\n",
    "* In deep learning frameworks, each parameter tensor has an attribute called `requires_grad` (typically a boolean).\n",
    "\n",
    "* `p.requires_grad = True` means that the gradients for this parameter will be computed during the backward pass of backpropagation, and thus, this parameter will be updated by the optimizer during training. These are the \"trainable\" parameters.\n",
    "\n",
    "* `p.requires_grad = False` means that this parameter is frozen. Its gradients will not be computed, and its value will not be updated during training. Examples of such parameters include:\n",
    "    * Parameters in a pre-trained model that you want to use as a fixed feature extractor (as seen with `vae.eval()` and `text_encoder.eval()` in your original code, which often implicitly sets `requires_grad` to False for their parameters, or you explicitly set it to False when loading/defining them).\n",
    "    \n",
    "    * Buffers in a model (like running means and variances in Batch Normalization layers), which are part of the model's state but are not \"trained\" in the same way weights and biases are, and typically don't have `requires_grad=True`.\n",
    "\n",
    "* By including `if p.requires_grad`, the function ensures that only trainable parameters are counted. This is important because you often want to know how many parameters your optimizer needs to manage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0483c282",
   "metadata": {},
   "source": [
    "## 4ï¸âƒ£ Data Preprocessing and Custom Dataset\n",
    "\n",
    "**Task**: Create custom dataset class and preprocessing pipeline.\n",
    "\n",
    "**Requirements**:\n",
    "- Resize images to 512x512 resolution\n",
    "- Normalize images to [-1, 1] range for VAE\n",
    "- Tokenize text descriptions\n",
    "- Handle data augmentation appropriately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "398c56c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 1221\n",
      "Sample batch keys: dict_keys(['pixel_values', 'input_ids'])\n",
      "Pixel values shape: torch.Size([1, 3, 512, 512])\n",
      "Input IDs shape: torch.Size([1, 77])\n"
     ]
    }
   ],
   "source": [
    "# TODO: Create NarutoDataset class inheriting from torch.utils.data.Dataset\n",
    "class NarutoDataset(Dataset):\n",
    "    # TODO: In __init__(self, dataset, tokenizer, size=512):\n",
    "    def __init__(self, dataset, tokenizer, size=512):\n",
    "        #       - Store dataset, tokenizer, and image size\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.size = size\n",
    "        #       - Define image transforms:\n",
    "        #         * Resize to (size, size)\n",
    "        #         * Random horizontal flip for augmentation\n",
    "        #         * ToTensor()\n",
    "        #         * Normalize with mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]\n",
    "        self.transforms = transforms.Compose([\n",
    "            transforms.Resize((size, size)),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n",
    "            transforms.RandomAffine(degrees=10, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5], [0.5]),\n",
    "        ])\n",
    "        # I added ColorJitter and RandomAffine for additional augmentation.\n",
    "\n",
    "    # TODO: Implement __len__ to return dataset length\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    # TODO: Implement __getitem__ to:\n",
    "    def __getitem__(self, idx):\n",
    "        #       - Get image and text from dataset\n",
    "        sample = self.dataset[idx]\n",
    "        image = sample['image'].convert(\"RGB\")\n",
    "        text = sample['text']\n",
    "\n",
    "        #       - Apply transforms to image\n",
    "        pixel_values = self.transforms(image)\n",
    "\n",
    "        #       - Tokenize text with padding and truncation\n",
    "        input_ids = self.tokenizer(\n",
    "            text,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.tokenizer.model_max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        ).input_ids\n",
    "\n",
    "        #       - Return dict with 'pixel_values' and 'input_ids'\n",
    "        return {\"pixel_values\": pixel_values, \"input_ids\": input_ids.squeeze()}\n",
    "\n",
    "# TODO: Create train_dataset and train_dataloader\n",
    "train_dataset = NarutoDataset(ds['train'], tokenizer, size=IMG_SIZE)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# TODO: Print dataset info and test with one sample\n",
    "print(f\"Dataset size: {len(train_dataset)}\")\n",
    "sample_batch = next(iter(train_dataloader))\n",
    "print(\"Sample batch keys:\", sample_batch.keys())\n",
    "print(\"Pixel values shape:\", sample_batch['pixel_values'].shape)\n",
    "print(\"Input IDs shape:\", sample_batch['input_ids'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce59c221",
   "metadata": {},
   "source": [
    "## 5ï¸âƒ£ Training Setup and Loss Function\n",
    "\n",
    "**Task**: Set up the training components including optimizer and loss function.\n",
    "\n",
    "**Requirements**:\n",
    "- Create optimizer for UNet parameters only\n",
    "- Implement the diffusion loss (noise prediction loss)\n",
    "- Set up proper gradient scaling and mixed precision if needed\n",
    "- Configure learning rate scheduling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9d58ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The config attributes {'predict_epsilon': True} were passed to DDPMScheduler, but are not expected and will be ignored. Please verify your scheduler_config.json configuration file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training setup complete. Optimizer, noise scheduler, helper functions, and GradScaler are ready.\n"
     ]
    }
   ],
   "source": [
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "# TODO: Create optimizer for UNet parameters only:\n",
    "#       - optimizer = torch.optim.AdamW(unet.parameters(), lr=LEARNING_RATE)\n",
    "optimizer = AdamW(unet.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# TODO: Create noise scheduler for training (different from inference)\n",
    "#       - noise_scheduler = DDPMScheduler.from_pretrained(MODEL_ID, subfolder=\"scheduler\")\n",
    "noise_scheduler = DDPMScheduler.from_pretrained(MODEL_ID, subfolder=\"scheduler\")\n",
    "# DDPMScheduler is a foundational scheduler used primarily for training\n",
    "# while PNDMScheduler is a more advanced scheduler designed for fast and high-quality inference (image generation).\n",
    "\n",
    "# TODO: Define helper functions:\n",
    "#       - encode_text(text_input): tokenize and encode text to embeddings\n",
    "#       - encode_image(image): encode image to latent space using VAE\n",
    "#       - decode_latent(latent): decode latent back to image using VAE\n",
    "@torch.no_grad()\n",
    "def encode_text(text_input_ids):\n",
    "    \"\"\"Tokenize and encode text to embeddings.\"\"\"\n",
    "    return text_encoder(text_input_ids.to(device))[0]\n",
    "\n",
    "@torch.no_grad()\n",
    "def encode_image(image_pixels):\n",
    "    \"\"\"Encode image to latent space using VAE.\"\"\"\n",
    "    h = vae.encode(image_pixels.to(device)).latent_dist\n",
    "    latents = h.sample()\n",
    "    return latents * vae.config.scaling_factor\n",
    "\n",
    "@torch.no_grad()\n",
    "def decode_latent(latents):\n",
    "    \"\"\"Decode latent back to image using VAE.\"\"\"\n",
    "    latents = 1 / vae.config.scaling_factor * latents\n",
    "    image = vae.decode(latents).sample\n",
    "    return image\n",
    "\n",
    "# TODO: Set up gradient scaler for mixed precision training (optional)\n",
    "scaler = torch.amp.GradScaler(\"cuda\", init_scale=2**16, growth_interval=1000, growth_factor=2.0)\n",
    "# è¿™é‡Œæˆ‘ä»¬ä½¿ç”¨ GradScaler æ¥æ”¯æŒæ··åˆç²¾åº¦è®­ç»ƒï¼Œè¿™å¯ä»¥æé«˜è®­ç»ƒé€Ÿåº¦å¹¶å‡å°‘å†…å­˜ä½¿ç”¨ã€‚\n",
    "# ä½†æ˜¯æ³¨æ„ï¼Œæˆ‘ä»¬ä¸‹é¢çš„è®­ç»ƒå¾ªç¯ä¸­ä¼šéœ€è¦ä½¿ç”¨ autocast æ¥è‡ªåŠ¨å¤„ç†æ··åˆç²¾åº¦ã€‚\n",
    "\n",
    "# TODO: Initialize training tracking variables\n",
    "losses = []\n",
    "print(\"Training setup complete. Optimizer, noise scheduler, helper functions, and GradScaler are ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d14f9f9",
   "metadata": {},
   "source": [
    "`Gradient Scaler` **(æ¢¯åº¦ç¼©æ”¾å™¨) æ˜¯ä¸€ä¸ªç”¨æ¥è§£å†³æ··åˆç²¾åº¦è®­ç»ƒä¸­â€œæ•°å€¼ä¸‹æº¢â€ (Underflow) é—®é¢˜çš„å·¥å…·ï¼Œå®ƒèƒ½ä¿è¯æ¨¡å‹åœ¨ä½ç²¾åº¦æµ®ç‚¹æ•°ï¼ˆå¦‚FP16ï¼‰ä¸‹ä¾ç„¶å¯ä»¥ç¨³å®šåœ°è®­ç»ƒ**ã€‚\n",
    "\n",
    "1. **ä»€ä¹ˆæ˜¯â€œæ··åˆç²¾åº¦è®­ç»ƒâ€ (Mixed Precision Training)ï¼Ÿ**\n",
    "\n",
    "    åœ¨æ·±åº¦å­¦ä¹ ä¸­ï¼Œæˆ‘ä»¬é€šå¸¸ä½¿ç”¨32ä½æµ®ç‚¹æ•°ï¼ˆFP32ï¼Œæˆ–ç§°å•ç²¾åº¦ï¼‰æ¥å­˜å‚¨å’Œè®¡ç®—æ¨¡å‹çš„æƒé‡ã€æ¢¯åº¦ç­‰æ•°æ®ã€‚\n",
    "\n",
    "    æ··åˆç²¾åº¦è®­ç»ƒåˆ™æ˜¯æŒ‡åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼ŒåŒæ—¶ä½¿ç”¨16ä½æµ®ç‚¹æ•°ï¼ˆFP16ï¼Œæˆ–ç§°åŠç²¾åº¦ï¼‰å’Œ32ä½æµ®ç‚¹æ•°ï¼ˆFP32ï¼‰ã€‚\n",
    "\n",
    "    ä¼˜ç‚¹ï¼š\n",
    "\n",
    "    * **é€Ÿåº¦æ›´å¿«**ï¼šç°ä»£çš„GPUï¼ˆå°¤å…¶æ˜¯NVIDIAå¸¦æœ‰Tensor Coresçš„æ˜¾å¡ï¼‰è®¡ç®—FP16çš„é€Ÿåº¦è¿œè¶…FP32ã€‚\n",
    "\n",
    "    * **æ˜¾å­˜æ›´å°‘**ï¼šFP16å ç”¨çš„å­˜å‚¨ç©ºé—´æ˜¯FP32çš„ä¸€åŠï¼Œè¿™æ„å‘³ç€ä½ å¯ä»¥ç”¨æ›´å¤§çš„æ¨¡å‹ã€æ›´å¤§çš„æ‰¹é‡ï¼ˆbatch sizeï¼‰è¿›è¡Œè®­ç»ƒã€‚\n",
    "\n",
    "    ç¼ºç‚¹/æŒ‘æˆ˜ï¼š\n",
    "\n",
    "    * **æ•°å€¼èŒƒå›´æ›´å°**ï¼šFP16èƒ½è¡¨ç¤ºçš„æ•°å€¼èŒƒå›´æ¯”FP32å°å¾—å¤šã€‚å¦‚æœä¸€ä¸ªæ•°éå¸¸å°ï¼Œè¶…å‡ºäº†FP16çš„è¡¨ç¤ºèŒƒå›´ï¼Œå®ƒå°±ä¼šè¢«å½“ä½œ**é›¶**ï¼Œè¿™ç§æƒ…å†µç§°ä¸ºâ€œ**æ•°å€¼ä¸‹æº¢**â€(Numerical Underflow)ã€‚\n",
    "\n",
    "2. **ä¸ºä»€ä¹ˆâ€œæ•°å€¼ä¸‹æº¢â€æ˜¯ä¸ªå¤§é—®é¢˜ï¼Ÿ**\n",
    "\n",
    "    åœ¨æ¨¡å‹è®­ç»ƒçš„åå‘ä¼ æ’­ï¼ˆbackpropagationï¼‰è¿‡ç¨‹ä¸­ï¼Œè®¡ç®—å‡ºçš„æ¢¯åº¦ (gradients) å¯èƒ½éå¸¸å°ã€‚å¦‚æœä½¿ç”¨FP16ï¼Œè¿™äº›å¾®å°çš„æ¢¯åº¦å€¼å¾ˆå®¹æ˜“å› ä¸ºâ€œæ•°å€¼ä¸‹æº¢â€è€Œå˜æˆé›¶ã€‚\n",
    "\n",
    "    ä¸€æ—¦æ¢¯åº¦å˜æˆé›¶ï¼Œæ¨¡å‹å°±æ— æ³•ä»è¿™äº›ä¿¡æ¯ä¸­å­¦åˆ°ä»»ä½•ä¸œè¥¿ï¼Œå‚æ•°ä¹Ÿå°±ä¸ä¼šè¢«æ›´æ–°ã€‚è¿™å°±å¥½æ¯”è€å¸ˆç»™å­¦ç”Ÿåˆ’é‡ç‚¹ï¼Œä½†å£°éŸ³å¤ªå°ï¼Œå­¦ç”Ÿä¸€ä¸ªå­—ä¹Ÿå¬ä¸è§ï¼Œè‡ªç„¶ä¹Ÿå°±æ— æ³•å­¦ä¹ å’Œè¿›æ­¥ã€‚æœ€ç»ˆï¼Œè¿™ä¼šå¯¼è‡´æ¨¡å‹**æ— æ³•æ”¶æ•›æˆ–è®­ç»ƒå¤±è´¥**ã€‚\n",
    "\n",
    "3. `Gradient Scaler` **å¦‚ä½•è§£å†³è¿™ä¸ªé—®é¢˜ï¼Ÿ**\n",
    "    \n",
    "    `Gradient Scaler` (åœ¨PyTorchä¸­æ˜¯ `torch.cuda.amp.GradScaler`) é€šè¿‡ä¸€ä¸ªå·§å¦™çš„â€œæ”¾å¤§å†ç¼©å°â€çš„ç­–ç•¥æ¥è§£å†³é—®é¢˜ï¼š\n",
    "\n",
    "    å·¥ä½œæµç¨‹å¦‚ä¸‹ï¼š\n",
    "\n",
    "    1. **æ”¾å¤§æŸå¤± (Loss Scaling)**ï¼šåœ¨è®¡ç®—åå‘ä¼ æ’­ä¹‹å‰ï¼ŒGradScaler ä¼šå°†è®¡ç®—å‡ºçš„æŸå¤±å€¼ï¼ˆLossï¼‰ä¹˜ä»¥ä¸€ä¸ªå·¨å¤§çš„ç¼©æ”¾å› å­ï¼ˆä¾‹å¦‚ 65536.0ï¼‰ã€‚\n",
    "\n",
    "    2. **è®¡ç®—æ”¾å¤§çš„æ¢¯åº¦ (Scaled Backward Pass)**ï¼šå½“ä½¿ç”¨è¿™ä¸ªè¢«æ”¾å¤§äº†çš„æŸå¤±å€¼è¿›è¡Œåå‘ä¼ æ’­æ—¶ï¼Œæ ¹æ®é“¾å¼æ³•åˆ™ï¼Œæ‰€æœ‰è®¡ç®—å‡ºçš„æ¢¯åº¦ä¹Ÿä¼šè¢«åŒæ ·åœ°æ”¾å¤§ã€‚\n",
    "\n",
    "        è¿™æ ·ä¸€æ¥ï¼Œé‚£äº›åŸæœ¬éå¸¸å¾®å°çš„ã€å¯èƒ½ä¼šä¸‹æº¢çš„æ¢¯åº¦ï¼Œç°åœ¨è¢«â€œæŠ¬å‡â€åˆ°äº†FP16å¯ä»¥å®‰å…¨è¡¨ç¤ºçš„èŒƒå›´å†…ï¼Œé¿å…äº†ä¿¡æ¯ä¸¢å¤±ã€‚\n",
    "\n",
    "    3. **ç¼©å°æ¢¯åº¦ (Unscaling)**ï¼šæ¢¯åº¦è™½ç„¶å®‰å…¨äº†ï¼Œä½†å®ƒä»¬æ˜¯â€œè™šé«˜â€çš„ï¼Œä¸èƒ½ç›´æ¥ç”¨æ¥æ›´æ–°æ¨¡å‹æƒé‡ã€‚å› æ­¤ï¼Œåœ¨ä¼˜åŒ–å™¨ï¼ˆOptimizerï¼‰æ›´æ–°æƒé‡ä¹‹å‰ï¼Œ`GradScaler` ä¼šå°†è¿™äº›è¢«æ”¾å¤§çš„æ¢¯åº¦**é™¤ä»¥åŒä¸€ä¸ªç¼©æ”¾å› å­**ï¼Œå°†å®ƒä»¬æ¢å¤åˆ°åŸå§‹çš„ã€æ­£ç¡®çš„æ•°å€¼ã€‚\n",
    "\n",
    "    4. **åŠ¨æ€è°ƒæ•´ç¼©æ”¾å› å­ (Dynamic Scaling)ï¼š** `GradScaler`éå¸¸æ™ºèƒ½ï¼Œå®ƒä¼šåŠ¨æ€è°ƒæ•´è¿™ä¸ªç¼©æ”¾å› å­ã€‚\n",
    "\n",
    "        å¦‚æœæ¢¯åº¦åœ¨æ”¾å¤§åå‡ºç°äº†æ— ç©·å¤§ï¼ˆ`inf`ï¼‰æˆ–éæ•°å­—ï¼ˆ`NaN`ï¼‰ï¼ˆè¯´æ˜ç¼©æ”¾å› å­å¤ªå¤§ï¼Œå¯¼è‡´â€œæ•°å€¼ä¸Šæº¢â€äº†ï¼‰ï¼Œ`GradScaler` ä¼šè·³è¿‡è¿™æ¬¡çš„å‚æ•°æ›´æ–°ï¼Œå¹¶åœ¨ä¸‹ä¸€æ¬¡è¿­ä»£æ—¶å‡å°ç¼©æ”¾å› å­ã€‚\n",
    "\n",
    "        å¦‚æœè¿ç»­å¾ˆå¤šæ¬¡è¿­ä»£éƒ½æ²¡æœ‰å‡ºç°é—®é¢˜ï¼Œå®ƒä¼šå°è¯•å¢å¤§ç¼©æ”¾å› å­ï¼Œä»¥åˆ©ç”¨FP16æ›´å¹¿çš„åŠ¨æ€èŒƒå›´ã€‚\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f156d0",
   "metadata": {},
   "source": [
    "`@torch.no_grad()` æ˜¯ä¸€ä¸ª PyTorch ä¸­çš„**è£…é¥°å™¨ (decorator)**ï¼Œå®ƒçš„ä½œç”¨éå¸¸æ˜ç¡®å’Œé‡è¦ï¼š**åœ¨è¯¥è£…é¥°å™¨ä¸‹çš„å‡½æ•°è¿è¡Œæ—¶ï¼Œä¸´æ—¶ç¦ç”¨æ¢¯åº¦è®¡ç®—**ã€‚\n",
    "\n",
    "ç®€å•æ¥è¯´ï¼Œå½“ä»£ç è¢« `@torch.no_grad()` åŒ…è£¹æ—¶ï¼ŒPyTorch å°±ä¸ä¼šå»è¿½è¸ªå’Œè®°å½•ä»»ä½•å¼ é‡ï¼ˆTensorï¼‰çš„æ“ä½œå†å²ï¼Œå› æ­¤ä¹Ÿå°±æ— æ³•ä¸ºè¿™äº›æ“ä½œè®¡ç®—æ¢¯åº¦ã€‚\n",
    "\n",
    "è¿™ä¼šå¸¦æ¥ä¸¤ä¸ªæ ¸å¿ƒçš„å¥½å¤„ï¼š\n",
    "\n",
    "1. **èŠ‚çœæ˜¾å­˜/å†…å­˜**ï¼šå› ä¸ºä¸éœ€è¦å­˜å‚¨è®¡ç®—å›¾ï¼ˆcomputation graphï¼‰å’Œä¸­é—´çŠ¶æ€æ¥ä¸ºåå‘ä¼ æ’­åšå‡†å¤‡ï¼Œæ‰€ä»¥ä¼šæ˜¾è‘—å‡å°‘å†…å­˜çš„æ¶ˆè€—ã€‚\n",
    "\n",
    "2. **åŠ å¿«è®¡ç®—é€Ÿåº¦**ï¼šç”±äºçœå»äº†è¿½è¸ªæ“ä½œçš„å¼€é”€ï¼Œä»£ç çš„æ‰§è¡Œé€Ÿåº¦ä¼šæ›´å¿«ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2589ce",
   "metadata": {},
   "source": [
    "## 6ï¸âƒ£ Training Loop Implementation\n",
    "\n",
    "**Task**: Implement the main training loop for diffusion model finetuning.\n",
    "\n",
    "**Requirements**:\n",
    "- Encode images to latent space using VAE\n",
    "- Add noise to latents according to diffusion schedule\n",
    "- Predict noise using UNet conditioned on text\n",
    "- Compute loss between predicted and actual noise\n",
    "- Update UNet parameters via backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3b9e5c",
   "metadata": {},
   "source": [
    "åœ¨Stable Diffusionçš„è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨ **MSE Loss (Mean Squared Error, å‡æ–¹è¯¯å·®æŸå¤±)**ï¼Œæ˜¯å› ä¸ºè¿™ä¸ªä»»åŠ¡çš„æœ¬è´¨æ˜¯ä¸€ä¸ª**å›å½’é—®é¢˜**ï¼Œè€ŒMSEæ˜¯è§£å†³è¿™ç±»é—®é¢˜çš„ç»å…¸ä¸”é«˜æ•ˆçš„å·¥å…·ã€‚\n",
    "\n",
    "å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬è®­ç»ƒçš„ç›®æ ‡æ˜¯**è®©æ¨¡å‹ç²¾ç¡®åœ°é¢„æµ‹å‡ºæˆ‘ä»¬å½“åˆæ·»åŠ çš„å™ªå£°**ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fcf29e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training with mixed precision...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0eebfdcf24544455a1c445f6061b3b28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/1221 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rh/2t3sy5bx13nctf2n07gkzskh0000gn/T/ipykernel_2187/1205080145.py:18: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(\"mps\"):\n",
      "/opt/anaconda3/lib/python3.12/site-packages/torch/amp/autocast_mode.py:266: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Cannot convert a MPS Tensor to float64 dtype as the MPS framework doesn't support float64. Please use float32 instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 42\u001b[0m\n\u001b[1;32m     40\u001b[0m scaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# ä½¿ç”¨ scaler æ¥æ‰§è¡Œä¼˜åŒ–å™¨æ­¥éª¤\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m scaler\u001b[38;5;241m.\u001b[39mstep(optimizer)\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# æ›´æ–° scaler ä¸ºä¸‹ä¸€æ¬¡è¿­ä»£åšå‡†å¤‡\u001b[39;00m\n\u001b[1;32m     44\u001b[0m scaler\u001b[38;5;241m.\u001b[39mupdate()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/amp/grad_scaler.py:455\u001b[0m, in \u001b[0;36mGradScaler.step\u001b[0;34m(self, optimizer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    452\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m retval\n\u001b[1;32m    454\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstage\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m OptState\u001b[38;5;241m.\u001b[39mREADY:\n\u001b[0;32m--> 455\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munscale_(optimizer)\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[1;32m    458\u001b[0m     \u001b[38;5;28mlen\u001b[39m(optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    459\u001b[0m ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo inf checks were recorded for this optimizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    461\u001b[0m retval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_opt_step(optimizer, optimizer_state, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/amp/grad_scaler.py:339\u001b[0m, in \u001b[0;36mGradScaler.unscale_\u001b[0;34m(self, optimizer)\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[38;5;66;03m# FP32 division can be imprecise for certain compile options, so we carry out the reciprocal in FP64.\u001b[39;00m\n\u001b[1;32m    338\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_scale \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 339\u001b[0m inv_scale \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_scale\u001b[38;5;241m.\u001b[39mdouble()\u001b[38;5;241m.\u001b[39mreciprocal()\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m    340\u001b[0m found_inf \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfull((), \u001b[38;5;241m0.0\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_scale\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    342\u001b[0m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_unscale_grads_(\n\u001b[1;32m    343\u001b[0m     optimizer, inv_scale, found_inf, \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    344\u001b[0m )\n",
      "\u001b[0;31mTypeError\u001b[0m: Cannot convert a MPS Tensor to float64 dtype as the MPS framework doesn't support float64. Please use float32 instead."
     ]
    }
   ],
   "source": [
    "# TODO: Implement training loop:\n",
    "print(\"Starting training with mixed precision...\")\n",
    "unet.train()  # Set UNet to training mode\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    progress_bar = tqdm(total=len(train_dataloader), desc=f\"Epoch {epoch + 1}\")\n",
    "    \n",
    "    #       For each batch in train_dataloader:\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        #           - Get images and text from batch\n",
    "        pixel_values = batch[\"pixel_values\"] \n",
    "        input_ids = batch[\"input_ids\"]\n",
    "\n",
    "        # Clear gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # ä½¿ç”¨ autocast ä¸Šä¸‹æ–‡ç®¡ç†å™¨è¿›è¡Œæ··åˆç²¾åº¦å‰å‘ä¼ æ’­\n",
    "        with autocast(\"cuda\"):\n",
    "            #           - Encode images to latent space using VAE\n",
    "            latents = encode_image(pixel_values)\n",
    "\n",
    "            #           - Encode text to embeddings using text encoder\n",
    "            encoder_hidden_states = encode_text(input_ids)\n",
    "            \n",
    "            #           - Sample random timesteps for diffusion\n",
    "            noise = torch.randn_like(latents) # ç”Ÿæˆä¸€ä¸ªä¸latentså¼ é‡å½¢çŠ¶å®Œå…¨ç›¸åŒï¼Œä½†å¡«å……äº†æ ‡å‡†æ­£æ€åˆ†å¸ƒï¼ˆé«˜æ–¯å™ªå£°ï¼‰éšæœºæ•°çš„å™ªå£°å¼ é‡ã€‚\n",
    "            timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (latents.shape[0],), device=latents.device).long()\n",
    "            \n",
    "            #           - Add noise to latents according to schedule\n",
    "            noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n",
    "            \n",
    "            #           - Predict noise using UNet with text conditioning\n",
    "            noise_pred = unet(noisy_latents, timesteps, encoder_hidden_states, return_dict=False)[0]\n",
    "\n",
    "            #           - Compute MSE loss between predicted and actual noise\n",
    "            loss = F.mse_loss(noise_pred, noise)\n",
    "        \n",
    "        #           - Backpropagate and update UNet parameters using the GradScaler\n",
    "        # ä½¿ç”¨ scaler æ¥ç¼©æ”¾æŸå¤±å¹¶è¿›è¡Œåå‘ä¼ æ’­\n",
    "        scaler.scale(loss).backward()\n",
    "        # ä½¿ç”¨ scaler æ¥æ‰§è¡Œä¼˜åŒ–å™¨æ­¥éª¤\n",
    "        scaler.step(optimizer)\n",
    "        # æ›´æ–° scaler ä¸ºä¸‹ä¸€æ¬¡è¿­ä»£åšå‡†å¤‡\n",
    "        scaler.update()\n",
    "        \n",
    "        #           - Track and display training progress\n",
    "        losses.append(loss.item())\n",
    "        progress_bar.set_postfix(loss=loss.item())\n",
    "        progress_bar.update(1)\n",
    "\n",
    "    progress_bar.close()\n",
    "\n",
    "# TODO: Save model checkpoints periodically\n",
    "# unet.save_pretrained(\"naruto_finetuned_unet_mixed_precision\")\n",
    "print(\"Training finished.\")\n",
    "\n",
    "# TODO: Display loss curves and training statistics\n",
    "# This will be done in the evaluation section (Block 9)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d398bffa",
   "metadata": {},
   "source": [
    "## 7ï¸âƒ£ Inference Pipeline Setup\n",
    "\n",
    "**Task**: Create inference pipeline for text-to-image generation.\n",
    "\n",
    "**Requirements**:\n",
    "- Set up complete diffusion pipeline with trained UNet\n",
    "- Configure scheduler for inference (100 steps)\n",
    "- Implement text-to-image generation function\n",
    "- Handle classifier-free guidance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f5c4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create inference pipeline:\n",
    "#       - Set all models to eval mode\n",
    "#       - Create StableDiffusionPipeline with trained components\n",
    "#       - Configure scheduler for inference\n",
    "# å¯¼å…¥ StableDiffusionPipeline ç±»\n",
    "from diffusers import StableDiffusionPipeline\n",
    "\n",
    "#       - Set all models to eval mode\n",
    "#       - Create StableDiffusionPipeline with trained components\n",
    "# æˆ‘ä»¬å°†æ‰€æœ‰å¾®è°ƒè¿‡çš„ï¼ˆunetï¼‰å’Œæœªæ”¹åŠ¨çš„ï¼ˆvae, text_encoderç­‰ï¼‰ç»„ä»¶åŠ è½½åˆ°ä¸€ä¸ªPipelineä¸­\n",
    "# è¿™æ˜¯è¿›è¡Œæ¨ç†çš„æ¨èæ–¹æ³•ï¼Œä»£ç æ›´ç®€æ´ä¸”ä¸æ˜“å‡ºé”™ã€‚\n",
    "pipeline = StableDiffusionPipeline(\n",
    "    vae=vae,\n",
    "    text_encoder=text_encoder,\n",
    "    tokenizer=tokenizer,\n",
    "    unet=unet,\n",
    "    scheduler=scheduler,\n",
    "    safety_checker=None, # å°å‹æ¨¡å‹é€šå¸¸æ²¡æœ‰å®‰å…¨æ£€æŸ¥å™¨\n",
    "    feature_extractor=None,\n",
    "    requires_safety_checker=False,\n",
    ")\n",
    "# StableDiffusionPipeline å†…éƒ¨ä¼šè‡ªåŠ¨å¤„ç† Set all models to eval modeï¼\n",
    "\n",
    "# å°†æ•´ä¸ª pipeline ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡\n",
    "pipeline.to(device)\n",
    "print(\"StableDiffusionPipeline created and moved to device.\")\n",
    "\n",
    "#       - Configure scheduler for inference\n",
    "# Pipeline ä¼šè‡ªåŠ¨å¤„ç†æ¨ç†æ—¶ scheduler çš„é…ç½®ï¼Œæˆ‘ä»¬åªéœ€åœ¨è°ƒç”¨æ—¶ä¼ å…¥æ­¥æ•°ã€‚\n",
    "\n",
    "# TODO: Implement generate_image function that:\n",
    "#       - Takes text prompt as input\n",
    "#       - Encodes text to embeddings (handled by pipeline)\n",
    "#       - Starts with random noise (handled by pipeline)\n",
    "#       - Performs denoising for specified number of steps (handled by pipeline)\n",
    "#       - Decodes final latent to image (handled by pipeline)\n",
    "#       - Returns PIL image\n",
    "@torch.no_grad()\n",
    "def generate_image(prompt, num_inference_steps, guidance_scale, seed=None):\n",
    "    # ä½¿ç”¨ç”Ÿæˆå™¨ä»¥ç¡®ä¿åœ¨ä½¿ç”¨ç§å­æ—¶ç»“æœå¯å¤ç°\n",
    "    generator = torch.manual_seed(seed) if seed is not None else None\n",
    "    \n",
    "    # è°ƒç”¨ pipeline ç”Ÿæˆå›¾åƒ\n",
    "    # pipeline ä¼šè‡ªåŠ¨å¤„ç†æ–‡æœ¬ç¼–ç ã€CFGã€é™å™ªå¾ªç¯å’ŒVAEè§£ç \n",
    "    result = pipeline(\n",
    "        prompt=prompt,\n",
    "        num_inference_steps=num_inference_steps,\n",
    "        guidance_scale=guidance_scale,\n",
    "        generator=generator\n",
    "    )\n",
    "    \n",
    "    # ä»ç»“æœä¸­è·å–å›¾åƒ\n",
    "    image = result.images[0]\n",
    "    return image\n",
    "\n",
    "# TODO: Set up proper inference configuration:\n",
    "#       - num_inference_steps = INFERENCE_STEPS\n",
    "#       - guidance_scale = GUIDANCE_SCALE\n",
    "#       - Enable safety checker if desired (already disabled in pipeline)\n",
    "print(f\"Inference function 'generate_image' is ready.\")\n",
    "print(f\"Default inference steps: {INFERENCE_STEPS}, Guidance scale: {GUIDANCE_SCALE}\")\n",
    "# åœ¨æœ€å¼€å§‹çš„Define configuration parameterséƒ¨åˆ†å·²ç»å®šä¹‰äº† INFERENCE_STEPS å’Œ GUIDANCE_SCALEã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b2289b",
   "metadata": {},
   "source": [
    "**Pipeline (ç®¡é“)**\n",
    "\n",
    "åœ¨Hugging Face `diffusers` åº“çš„è¯­å¢ƒä¸‹ï¼Œ`Pipeline` (ç®¡é“) æ˜¯ä¸€ä¸ª**é«˜çº§ã€ä¸€ä½“åŒ–çš„å·¥å…·ï¼Œå®ƒå°†è¿è¡Œä¸€ä¸ªå¤æ‚æ¨¡å‹ï¼ˆå¦‚Stable Diffusionï¼‰æ‰€éœ€çš„æ‰€æœ‰ç»„ä»¶å’Œæ­¥éª¤éƒ½å°è£…æ‰“åŒ…åœ¨äº†ä¸€èµ·**ã€‚\n",
    "\n",
    "ä½ å¯ä»¥æŠŠå®ƒæƒ³è±¡æˆä¸€ä¸ªâ€œ**å…¨è‡ªåŠ¨çš„å›¾åƒç”Ÿæˆå·¥å‚**â€ã€‚ä½ ä¸éœ€è¦å…³å¿ƒå·¥å‚å†…éƒ¨çš„å„ä¸ªè½¦é—´ï¼ˆæ¨¡å‹ç»„ä»¶ï¼‰æ˜¯å¦‚ä½•ååŒå·¥ä½œçš„ï¼Œä½ åªéœ€è¦å‘å·¥å‚ä¸‹è®¢å•ï¼ˆæä¾›ä¸€ä¸ªæ–‡æœ¬æç¤º `prompt`ï¼‰ï¼Œå·¥å‚å°±èƒ½è‡ªåŠ¨å®Œæˆæ‰€æœ‰å·¥åºï¼Œæœ€ç»ˆäº¤ä»˜ç»™ä½ æˆå“ï¼ˆä¸€å¼ å›¾ç‰‡ï¼‰ã€‚\n",
    "\n",
    "ä¸€ä¸ªå…¸å‹çš„ `StableDiffusionPipeline` åŒ…å«ä»¥ä¸‹è¿™äº›æ ¸å¿ƒç»„ä»¶ï¼š\n",
    "\n",
    "1. **Tokenizer (åˆ†è¯å™¨)**ï¼šè´Ÿè´£æ¥æ”¶ä½ è¾“å…¥çš„æ–‡æœ¬ï¼ˆå¦‚ \"a photo of an astronaut riding a horse on mars\"ï¼‰ï¼Œå¹¶å°†å…¶è½¬æ¢æˆæ¨¡å‹èƒ½ç†è§£çš„æ•°å­—IDã€‚\n",
    "\n",
    "2. **Text Encoder (æ–‡æœ¬ç¼–ç å™¨)**ï¼šæ¥æ”¶åˆ†è¯å™¨è¾“å‡ºçš„æ•°å­—IDï¼Œå¹¶å°†å…¶è½¬æ¢æˆåŒ…å«ä¸°å¯Œè¯­ä¹‰ä¿¡æ¯çš„å‘é‡ï¼ˆembeddingsï¼‰ã€‚è¿™æ˜¯æ¨¡å‹ç†è§£ä½ â€œæƒ³è¦ä»€ä¹ˆâ€çš„å…³é”®ã€‚\n",
    "\n",
    "3. **UNet (æ ¸å¿ƒé™å™ªæ¨¡å‹)**ï¼šè¿™æ˜¯æ‰©æ•£æ¨¡å‹çš„å¿ƒè„ã€‚å®ƒæ¥æ”¶ä¸€ä¸ªéšæœºå™ªå£°å›¾å’Œæ–‡æœ¬ç¼–ç å™¨çš„è¾“å‡ºï¼Œç„¶ååœ¨å¤šä¸ªæ­¥éª¤ä¸­ï¼Œé€æ­¥å»é™¤å™ªå£°ï¼Œæœ€ç»ˆâ€œé›•åˆ»â€å‡ºç¬¦åˆæ–‡æœ¬æè¿°çš„å›¾åƒçš„æ½œç©ºé—´è¡¨ç¤ºã€‚ä½ åœ¨å¾®è°ƒï¼ˆfinetuningï¼‰æ—¶ï¼Œè®­ç»ƒçš„å°±æ˜¯è¿™ä¸ªç»„ä»¶ã€‚\n",
    "\n",
    "4. **VAE (Variational Autoencoder, å˜åˆ†è‡ªç¼–ç å™¨)**ï¼š\n",
    "\n",
    "    ç¼–ç ï¼šåœ¨è®­ç»ƒå¼€å§‹å‰ï¼Œå®ƒè´Ÿè´£å°†åŸå§‹å›¾ç‰‡å‹ç¼©åˆ°æ›´å°ã€æ›´é«˜æ•ˆçš„æ½œç©ºé—´ï¼ˆ`latents`ï¼‰ã€‚\n",
    "\n",
    "    è§£ç ï¼šåœ¨æ¨ç†ç»“æŸæ—¶ï¼Œå®ƒè´Ÿè´£å°†UNetç”Ÿæˆçš„ã€å¹²å‡€çš„æ½œç©ºé—´è¡¨ç¤ºè§£ç ï¼ˆâ€œæ”¾å¤§â€ï¼‰å›æˆ‘ä»¬èƒ½çœ‹åˆ°çš„æ­£å¸¸åƒç´ å›¾åƒã€‚\n",
    "\n",
    "5. **Scheduler (è°ƒåº¦å™¨)**ï¼šè´Ÿè´£ç®¡ç†æ•´ä¸ªé™å™ªè¿‡ç¨‹çš„â€œæ­¥è°ƒâ€ã€‚å®ƒå®šä¹‰äº†æ€»å…±æœ‰å¤šå°‘ä¸ªé™å™ªæ­¥éª¤ï¼Œä»¥åŠåœ¨æ¯ä¸ªæ­¥éª¤ä¸­åº”è¯¥å»é™¤å¤šå°‘å™ªå£°ã€‚ä¸åŒçš„è°ƒåº¦å™¨ï¼ˆå¦‚ `PNDM`, `DPM-Solver`ï¼‰æœ‰ä¸åŒçš„å»å™ªç­–ç•¥ï¼Œä¼šå½±å“ç”Ÿæˆé€Ÿåº¦å’Œå›¾ç‰‡è´¨é‡ã€‚\n",
    "\n",
    "**Pipeline çš„æ ¸å¿ƒä¼˜ç‚¹ï¼š**\n",
    "\n",
    "* **ä¾¿æ·æ€§**ï¼šå®ƒéšè—äº†æ‰€æœ‰å¤æ‚çš„å†…éƒ¨è°ƒç”¨æµç¨‹ã€‚ä½ åªéœ€è¦ä¸€è¡Œä»£ç  `pipeline(prompt)` å°±èƒ½ç”Ÿæˆå›¾ç‰‡ï¼Œè€Œä¸éœ€è¦æ‰‹åŠ¨è°ƒç”¨5ä¸ªä¸åŒçš„ç»„ä»¶å¹¶ä¼ é€’æ•°æ®ã€‚\n",
    "\n",
    "* **ä¸€è‡´æ€§ä¸æ­£ç¡®æ€§**ï¼šå®ƒèƒ½ç¡®ä¿æ‰€æœ‰ç»„ä»¶éƒ½ä»¥æ­£ç¡®çš„æ–¹å¼ï¼ˆä¾‹å¦‚ï¼Œåœ¨æ¨ç†æ—¶è‡ªåŠ¨è®¾ç½® `.eval()` æ¨¡å¼ï¼‰ååŒå·¥ä½œï¼Œå‡å°‘äº†ç”¨æˆ·å‡ºé”™çš„å¯èƒ½æ€§ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4bea33",
   "metadata": {},
   "source": [
    "## 8ï¸âƒ£ Generate Images with Dataset Prompts\n",
    "\n",
    "**Task**: Generate images using 5 prompts from the training dataset.\n",
    "\n",
    "**Requirements**:\n",
    "- Select 5 different text prompts from the dataset\n",
    "- Generate images for each prompt\n",
    "- Display results in a grid format\n",
    "- Show prompt text alongside generated images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95f99e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Select 5 prompts from training dataset:\n",
    "#       - Use different indices to get variety\n",
    "#       - Extract text descriptions\n",
    "dataset_prompts = [ds['train'][i]['text'] for i in [10, 100, 200, 300, 400]]\n",
    "\n",
    "#       - Set random seed for reproducibility\n",
    "generation_seed = 1337\n",
    "generated_images_dataset = []\n",
    "\n",
    "# TODO: Generate images for each dataset prompt:\n",
    "for prompt in dataset_prompts:\n",
    "    print(f\"Generating image for prompt: '{prompt}'\")\n",
    "    #       - Use generate_image function\n",
    "    #       - Set random seed for reproducibility\n",
    "    image = generate_image(\n",
    "        prompt,\n",
    "        num_inference_steps=INFERENCE_STEPS,\n",
    "        guidance_scale=GUIDANCE_SCALE,\n",
    "        seed=generation_seed\n",
    "    )\n",
    "    #       - Save generated images\n",
    "    generated_images_dataset.append(image)\n",
    "\n",
    "# TODO: Create visualization:\n",
    "#       - Display each prompt text\n",
    "#       - Show corresponding generated image\n",
    "#       - Use matplotlib subplot for clean layout\n",
    "#       - Add titles and proper formatting\n",
    "fig, axes = plt.subplots(1, 5, figsize=(25, 5))\n",
    "for i, (prompt, img) in enumerate(zip(dataset_prompts, generated_images_dataset)):\n",
    "    axes[i].imshow(img)\n",
    "    axes[i].set_title(f\"Prompt: \\\"{prompt[:40]}...\\\"\", fontsize=10)\n",
    "    axes[i].axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.suptitle(\"Generated Images from Training Dataset Prompts\", fontsize=16)\n",
    "\n",
    "# TODO: Display results in a 2x3 grid or similar arrangement\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecec78db",
   "metadata": {},
   "source": [
    "`guidance_scale` controls how strictly the model follows your text prompt.\n",
    "\n",
    "The `seed` (or random seed) is a number that initializes the model's random number generator."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ffe12b7",
   "metadata": {},
   "source": [
    "## 9ï¸âƒ£ Generate Images with Custom Prompts\n",
    "\n",
    "**Task**: Generate images using 5 custom prompts that you create.\n",
    "\n",
    "**Requirements**:\n",
    "- Write 5 creative prompts in Naruto style\n",
    "- Test different types of descriptions (characters, scenes, actions)\n",
    "- Generate and display results\n",
    "- Compare quality with dataset prompt results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ef5352",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define 5 custom prompts, for example:\n",
    "custom_prompts = [\n",
    "    \"A ninja with sharingan eyes, wearing a black cloak, standing in the rain, naruto style\",\n",
    "    \"A majestic nine-tailed fox spirit with glowing red chakra, naruto style\",\n",
    "    \"A beautiful kunoichi with pink hair, sad expression, cherry blossoms falling around her, naruto style\",\n",
    "    \"An epic battle between two powerful shinobis on top of a giant statue, naruto style\",\n",
    "    \"The hidden leaf village seen from the hokage rock at night, naruto style\"\n",
    "]\n",
    "\n",
    "#       - Use same generation parameters as before\n",
    "#       - Ensure consistent quality\n",
    "generation_seed = 42\n",
    "generated_images_custom = []\n",
    "\n",
    "# TODO: Generate images for each custom prompt:\n",
    "for prompt in custom_prompts:\n",
    "    print(f\"Generating image for custom prompt: '{prompt}'\")\n",
    "    image = generate_image(\n",
    "        prompt,\n",
    "        num_inference_steps=INFERENCE_STEPS,\n",
    "        guidance_scale=GUIDANCE_SCALE,\n",
    "        seed=generation_seed\n",
    "    )\n",
    "    generated_images_custom.append(image)\n",
    "\n",
    "# TODO: Create visualization for custom prompts:\n",
    "#       - Similar layout to dataset prompts\n",
    "#       - Show prompt text and generated image\n",
    "fig, axes = plt.subplots(1, 5, figsize=(25, 5))\n",
    "for i, (prompt, img) in enumerate(zip(custom_prompts, generated_images_custom)):\n",
    "    axes[i].imshow(img)\n",
    "    axes[i].set_title(f\"Prompt: \\\"{prompt[:40]}...\\\"\", fontsize=10)\n",
    "    axes[i].axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.suptitle(\"Generated Images from Custom Prompts\", fontsize=16)\n",
    "\n",
    "# TODO: Display all 5 custom prompt results\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c68346",
   "metadata": {},
   "source": [
    "## ğŸ”Ÿ Model Evaluation and Comparison\n",
    "\n",
    "**Task**: Evaluate and compare your results\n",
    "\n",
    "**Requirements**:\n",
    "- Compare generated images with original dataset images\n",
    "- Evaluate image quality, style consistency, and prompt adherence\n",
    "- Plot training progress and loss convergence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfcc58ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create comparison visualization:\n",
    "#       - Show original dataset images alongside generated ones\n",
    "#       - Compare style consistency\n",
    "#       - Evaluate prompt adherence\n",
    "print(\"Comparing original dataset images with generated images.\")\n",
    "indices_to_compare = [50, 150, 250, 350, 450]\n",
    "prompts_to_compare = [ds['train'][i]['text'] for i in indices_to_compare]\n",
    "original_images = [ds['train'][i]['image'].convert(\"RGB\") for i in indices_to_compare]\n",
    "generated_images_compare = []\n",
    "\n",
    "for prompt in prompts_to_compare:\n",
    "    print(f\"Generating for comparison: '{prompt}'\")\n",
    "    img = generate_image(prompt, num_inference_steps=INFERENCE_STEPS, guidance_scale=GUIDANCE_SCALE, seed=8888)\n",
    "    generated_images_compare.append(img)\n",
    "\n",
    "fig, axes = plt.subplots(2, 5, figsize=(25, 10))\n",
    "fig.suptitle('Original vs. Generated Image Comparison', fontsize=16)\n",
    "\n",
    "for i in range(5):\n",
    "    # Plot original images\n",
    "    axes[0, i].imshow(original_images[i])\n",
    "    axes[0, i].set_title(f\"Original: \\\"{prompts_to_compare[i][:30]}...\\\"\", fontsize=10)\n",
    "    axes[0, i].axis('off')\n",
    "    \n",
    "    # Plot generated images\n",
    "    axes[1, i].imshow(generated_images_compare[i])\n",
    "    axes[1, i].set_title(f\"Generated: \\\"{prompts_to_compare[i][:30]}...\\\"\", fontsize=10)\n",
    "    axes[1, i].axis('off')\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# TODO: Plot training loss curve:\n",
    "#       - Show loss progression over epochs\n",
    "#       - Analyze convergence behavior\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(losses)\n",
    "plt.title(\"Training Loss Convergence\")\n",
    "plt.xlabel(\"Training Steps\")\n",
    "plt.ylabel(\"MSE Loss\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(\"Evaluation complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936c8434",
   "metadata": {},
   "source": [
    "## ğŸ“ Evaluation Criteria\n",
    "\n",
    "Your homework will be evaluated based on:\n",
    "\n",
    "1. **Implementation Correctness (40%)**\n",
    "   - Proper stable diffusion pipeline setup\n",
    "   - Correct training loop implementation\n",
    "   - Working inference pipeline\n",
    "   - Appropriate use of VAE, UNet, text encoder, and scheduler\n",
    "\n",
    "2. **Training and Results (30%)**\n",
    "   - Model trains without errors\n",
    "   - Reasonable loss convergence\n",
    "   - Generated images show Naruto style characteristics\n",
    "   - Successful generation from both dataset and custom prompts\n",
    "\n",
    "3. **Code Quality (30%)**\n",
    "   - Clean, readable code with proper comments\n",
    "   - Efficient memory usage and error handling\n",
    "   - Proper tensor operations and device management\n",
    "   - Good visualization and presentation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
