{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b53b4dac",
   "metadata": {},
   "source": [
    "# Text-to-Image Generation using Stable Diffusion - Homework Assignment\n",
    "\n",
    "![Stable Diffusion Architecture](https://miro.medium.com/v2/resize:fit:1400/1*NpQ282NJdOfxUsYlwLJplA.png)\n",
    "\n",
    "In this homework, you will finetune a **Stable Diffusion** model to generate Naruto-style images from text descriptions. This involves working with the complete diffusion pipeline including VAE, UNet, text encoder, and scheduler.\n",
    "\n",
    "## 📌 Project Overview\n",
    "- **Task**: Text-to-Naruto image generation\n",
    "- **Architecture**: Stable Diffusion with UNet diffusion model\n",
    "- **Dataset**: Naruto-style dataset with text descriptions\n",
    "- **Goal**: Generate realistic Naruto-style images from text prompts\n",
    "\n",
    "## 📚 Learning Objectives\n",
    "By completing this assignment, you will:\n",
    "- Understand diffusion models and the stable diffusion pipeline\n",
    "- Learn to finetune pre-trained diffusion models\n",
    "- Work with VAE, UNet, text encoders, and schedulers\n",
    "- Practice text-to-image generation techniques\n",
    "- Handle memory constraints with large models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f022d71",
   "metadata": {},
   "source": [
    "## 1️⃣ Dataset Setup (PROVIDED)\n",
    "\n",
    "The Naruto-style dataset has been loaded for you. The dataset contains:\n",
    "- 1,221 training images with corresponding text descriptions\n",
    "- Each sample has an 'image' and 'text' field\n",
    "- Images are in various sizes and need to be resized to 512x512\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a58ad0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12508bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U datasets\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3eeecc38",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-12T00:57:23.121088Z",
     "start_time": "2025-07-12T00:57:22.525983Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset info: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['image', 'text'],\n",
      "        num_rows: 1221\n",
      "    })\n",
      "})\n",
      "Number of training samples: 1221\n",
      "\n",
      "Sample text: a man with dark hair and brown eyes, naruto style\n",
      "Image size: (1080, 1080)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load dataset without any cache directory specified\n",
    "ds = load_dataset(\"Alex-0402/naruto-style-dataset-with-text\")\n",
    "print(\"Dataset info:\", ds)\n",
    "print(\"Number of training samples:\", len(ds['train']))\n",
    "\n",
    "# Display a sample\n",
    "sample = ds['train'][0]\n",
    "print(\"\\nSample text:\", sample['text'])\n",
    "print(\"Image size:\", sample['image'].size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14dd0a00",
   "metadata": {},
   "source": [
    "## 2️⃣ Import Libraries and Configuration\n",
    "\n",
    "**Task**: Import all necessary libraries and set up configuration parameters.\n",
    "\n",
    "**Requirements**:\n",
    "- Import diffusers, transformers, and related libraries\n",
    "- Import PyTorch, PIL, numpy, and other utilities\n",
    "- Set random seeds for reproducibility\n",
    "- Configure hyperparameters for stable diffusion training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74428cb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPS device for Apple Silicon GPU acceleration.\n",
      "device now using: mps\n"
     ]
    }
   ],
   "source": [
    "# TODO: Import all necessary libraries:\n",
    "#       - torch, torch.nn.functional, torch.optim\n",
    "#       - diffusers (UNet2DConditionModel, AutoencoderKL, PNDMScheduler, DDPMScheduler)\n",
    "#       - transformers (CLIPTextModel, CLIPTokenizer)\n",
    "#       - PIL, numpy, matplotlib\n",
    "#       - torchvision.transforms\n",
    "#       - tqdm for progress bars\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from diffusers import UNet2DConditionModel, AutoencoderKL, PNDMScheduler, DDPMScheduler\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "from tqdm.auto import tqdm\n",
    "import random\n",
    "\n",
    "# TODO: Set random seeds for reproducibility (use seed=42)\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# TODO: Check device availability and print\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"device now using: {device}\")\n",
    "\n",
    "# TODO: Define configuration parameters:\n",
    "MODEL_ID = \"OFA-Sys/small-stable-diffusion-v0\"  # Smaller stable diffusion model\n",
    "IMG_SIZE = 512  # Image resolution\n",
    "BATCH_SIZE = 1  # Small batch size for memory constraints\n",
    "LEARNING_RATE = 1e-5  # Learning rate for finetuning\n",
    "NUM_EPOCHS = 6  # Number of training epochs\n",
    "INFERENCE_STEPS = 200  # Number of denoising steps during inference\n",
    "GUIDANCE_SCALE = 7.5  # Classifier-free guidance scale"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b488affe",
   "metadata": {},
   "source": [
    "计算机生成的“随机”其实是“**伪随机**” **(Pseudo-random)**。\n",
    "\n",
    "简单来说：随机种子是伪随机数生成算法的起始点。只要起始点相同，生成的“随机”数字序列就将完全一样。\n",
    "\n",
    "所以，设置随机种子有利于模型复现！"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2ff29e",
   "metadata": {},
   "source": [
    "## 3️⃣ Load Pre-trained Stable Diffusion Components\n",
    "\n",
    "**Task**: Load all components of the stable diffusion pipeline.\n",
    "\n",
    "**Requirements**:\n",
    "- Load VAE (Variational Autoencoder) for image encoding/decoding\n",
    "- Load UNet for the diffusion process\n",
    "- Load text encoder and tokenizer for text conditioning\n",
    "- Load noise scheduler for the diffusion process\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d2312cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error occurred while trying to fetch OFA-Sys/small-stable-diffusion-v0: OFA-Sys/small-stable-diffusion-v0 does not appear to have a file named diffusion_pytorch_model.safetensors.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n",
      "An error occurred while trying to fetch OFA-Sys/small-stable-diffusion-v0: OFA-Sys/small-stable-diffusion-v0 does not appear to have a file named diffusion_pytorch_model.safetensors.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n",
      "The config attributes {'predict_epsilon': True} were passed to PNDMScheduler, but are not expected and will be ignored. Please verify your scheduler_config.json configuration file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNet has 579,384,964 trainable parameters.\n",
      "VAE has 83,653,863 trainable parameters (frozen).\n",
      "Text Encoder has 123,060,480 trainable parameters (frozen).\n"
     ]
    }
   ],
   "source": [
    "# TODO: Load stable diffusion components:\n",
    "#       - vae = AutoencoderKL.from_pretrained(MODEL_ID, subfolder=\"vae\")\n",
    "#       - unet = UNet2DConditionModel.from_pretrained(MODEL_ID, subfolder=\"unet\")\n",
    "#       - text_encoder = CLIPTextModel.from_pretrained(MODEL_ID, subfolder=\"text_encoder\")\n",
    "#       - tokenizer = CLIPTokenizer.from_pretrained(MODEL_ID, subfolder=\"tokenizer\")\n",
    "#       - scheduler = PNDMScheduler.from_pretrained(MODEL_ID, subfolder=\"scheduler\")\n",
    "\n",
    "vae = AutoencoderKL.from_pretrained(MODEL_ID, subfolder=\"vae\")\n",
    "unet = UNet2DConditionModel.from_pretrained(MODEL_ID, subfolder=\"unet\")\n",
    "text_encoder = CLIPTextModel.from_pretrained(MODEL_ID, subfolder=\"text_encoder\")\n",
    "tokenizer = CLIPTokenizer.from_pretrained(MODEL_ID, subfolder=\"tokenizer\")\n",
    "scheduler = PNDMScheduler.from_pretrained(MODEL_ID, subfolder=\"scheduler\") \n",
    "# DDPMScheduler is a foundational scheduler used primarily for training\n",
    "# while PNDMScheduler is a more advanced scheduler designed for fast and high-quality inference (image generation).\n",
    "\n",
    "# TODO: Move models to device\n",
    "vae.to(device)\n",
    "unet.to(device)\n",
    "text_encoder.to(device)\n",
    "\n",
    "# TODO: Set VAE and text encoder to eval mode (only UNet will be trained)\n",
    "vae.eval()\n",
    "text_encoder.eval()\n",
    "\n",
    "# TODO: Print model information and parameter counts\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    # numel() is a method available on tensor objects (e.g., PyTorch tensors) that returns the total number of elements (or scalar values) in that tensor.\n",
    "\n",
    "print(f\"UNet has {count_parameters(unet):,} trainable parameters.\")\n",
    "print(f\"VAE has {count_parameters(vae):,} trainable parameters (frozen).\")\n",
    "print(f\"Text Encoder has {count_parameters(text_encoder):,} trainable parameters (frozen).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50328c67",
   "metadata": {},
   "source": [
    "The `scheduler` is responsible for **defining the diffusion process and how noise is added and removed at each step during the sampling (inference) process**.\n",
    "\n",
    "More specifically, the `scheduler` handles:\n",
    "\n",
    "1. **Noise Scheduling**: It determines the amount of noise to add or remove at each step of the denoising process.\n",
    "\n",
    "2. **Denoising Algorithm**: It implements the specific algorithm used to denoise the latent representation.\n",
    "\n",
    "3. **Timesteps**: It manages the timesteps (or noise levels) through which the model progressively denoises the latent representation.\n",
    "\n",
    "4. **Prediction Type**: It often handles whether the model is predicting the noise itself, the original image, or the velocity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc4f126",
   "metadata": {},
   "source": [
    "`if p.requires_grad`:\n",
    "* This is a crucial **filter** within the generator expression.\n",
    "\n",
    "* In deep learning frameworks, each parameter tensor has an attribute called `requires_grad` (typically a boolean).\n",
    "\n",
    "* `p.requires_grad = True` means that the gradients for this parameter will be computed during the backward pass of backpropagation, and thus, this parameter will be updated by the optimizer during training. These are the \"trainable\" parameters.\n",
    "\n",
    "* `p.requires_grad = False` means that this parameter is frozen. Its gradients will not be computed, and its value will not be updated during training. Examples of such parameters include:\n",
    "    * Parameters in a pre-trained model that you want to use as a fixed feature extractor (as seen with `vae.eval()` and `text_encoder.eval()` in your original code, which often implicitly sets `requires_grad` to False for their parameters, or you explicitly set it to False when loading/defining them).\n",
    "    \n",
    "    * Buffers in a model (like running means and variances in Batch Normalization layers), which are part of the model's state but are not \"trained\" in the same way weights and biases are, and typically don't have `requires_grad=True`.\n",
    "\n",
    "* By including `if p.requires_grad`, the function ensures that only trainable parameters are counted. This is important because you often want to know how many parameters your optimizer needs to manage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0483c282",
   "metadata": {},
   "source": [
    "## 4️⃣ Data Preprocessing and Custom Dataset\n",
    "\n",
    "**Task**: Create custom dataset class and preprocessing pipeline.\n",
    "\n",
    "**Requirements**:\n",
    "- Resize images to 512x512 resolution\n",
    "- Normalize images to [-1, 1] range for VAE\n",
    "- Tokenize text descriptions\n",
    "- Handle data augmentation appropriately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "398c56c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 1221\n",
      "Sample batch keys: dict_keys(['pixel_values', 'input_ids'])\n",
      "Pixel values shape: torch.Size([1, 3, 512, 512])\n",
      "Input IDs shape: torch.Size([1, 77])\n"
     ]
    }
   ],
   "source": [
    "# TODO: Create NarutoDataset class inheriting from torch.utils.data.Dataset\n",
    "class NarutoDataset(Dataset):\n",
    "    # TODO: In __init__(self, dataset, tokenizer, size=512):\n",
    "    def __init__(self, dataset, tokenizer, size=512):\n",
    "        #       - Store dataset, tokenizer, and image size\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.size = size\n",
    "        #       - Define image transforms:\n",
    "        #         * Resize to (size, size)\n",
    "        #         * Random horizontal flip for augmentation\n",
    "        #         * ToTensor()\n",
    "        #         * Normalize with mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]\n",
    "        self.transforms = transforms.Compose([\n",
    "            transforms.Resize((size, size)),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n",
    "            transforms.RandomAffine(degrees=10, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5], [0.5]),\n",
    "        ])\n",
    "        # I added ColorJitter and RandomAffine for additional augmentation.\n",
    "\n",
    "    # TODO: Implement __len__ to return dataset length\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    # TODO: Implement __getitem__ to:\n",
    "    def __getitem__(self, idx):\n",
    "        #       - Get image and text from dataset\n",
    "        sample = self.dataset[idx]\n",
    "        image = sample['image'].convert(\"RGB\")\n",
    "        text = sample['text']\n",
    "\n",
    "        #       - Apply transforms to image\n",
    "        pixel_values = self.transforms(image)\n",
    "\n",
    "        #       - Tokenize text with padding and truncation\n",
    "        input_ids = self.tokenizer(\n",
    "            text,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.tokenizer.model_max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        ).input_ids\n",
    "\n",
    "        #       - Return dict with 'pixel_values' and 'input_ids'\n",
    "        return {\"pixel_values\": pixel_values, \"input_ids\": input_ids.squeeze()}\n",
    "\n",
    "# TODO: Create train_dataset and train_dataloader\n",
    "train_dataset = NarutoDataset(ds['train'], tokenizer, size=IMG_SIZE)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# TODO: Print dataset info and test with one sample\n",
    "print(f\"Dataset size: {len(train_dataset)}\")\n",
    "sample_batch = next(iter(train_dataloader))\n",
    "print(\"Sample batch keys:\", sample_batch.keys())\n",
    "print(\"Pixel values shape:\", sample_batch['pixel_values'].shape)\n",
    "print(\"Input IDs shape:\", sample_batch['input_ids'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce59c221",
   "metadata": {},
   "source": [
    "## 5️⃣ Training Setup and Loss Function\n",
    "\n",
    "**Task**: Set up the training components including optimizer and loss function.\n",
    "\n",
    "**Requirements**:\n",
    "- Create optimizer for UNet parameters only\n",
    "- Implement the diffusion loss (noise prediction loss)\n",
    "- Set up proper gradient scaling and mixed precision if needed\n",
    "- Configure learning rate scheduling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9d58ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The config attributes {'predict_epsilon': True} were passed to DDPMScheduler, but are not expected and will be ignored. Please verify your scheduler_config.json configuration file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training setup complete. Optimizer, noise scheduler, helper functions, and GradScaler are ready.\n"
     ]
    }
   ],
   "source": [
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "# TODO: Create optimizer for UNet parameters only:\n",
    "#       - optimizer = torch.optim.AdamW(unet.parameters(), lr=LEARNING_RATE)\n",
    "optimizer = AdamW(unet.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# TODO: Create noise scheduler for training (different from inference)\n",
    "#       - noise_scheduler = DDPMScheduler.from_pretrained(MODEL_ID, subfolder=\"scheduler\")\n",
    "noise_scheduler = DDPMScheduler.from_pretrained(MODEL_ID, subfolder=\"scheduler\")\n",
    "# DDPMScheduler is a foundational scheduler used primarily for training\n",
    "# while PNDMScheduler is a more advanced scheduler designed for fast and high-quality inference (image generation).\n",
    "\n",
    "# TODO: Define helper functions:\n",
    "#       - encode_text(text_input): tokenize and encode text to embeddings\n",
    "#       - encode_image(image): encode image to latent space using VAE\n",
    "#       - decode_latent(latent): decode latent back to image using VAE\n",
    "@torch.no_grad()\n",
    "def encode_text(text_input_ids):\n",
    "    \"\"\"Tokenize and encode text to embeddings.\"\"\"\n",
    "    return text_encoder(text_input_ids.to(device))[0]\n",
    "\n",
    "@torch.no_grad()\n",
    "def encode_image(image_pixels):\n",
    "    \"\"\"Encode image to latent space using VAE.\"\"\"\n",
    "    h = vae.encode(image_pixels.to(device)).latent_dist\n",
    "    latents = h.sample()\n",
    "    return latents * vae.config.scaling_factor\n",
    "\n",
    "@torch.no_grad()\n",
    "def decode_latent(latents):\n",
    "    \"\"\"Decode latent back to image using VAE.\"\"\"\n",
    "    latents = 1 / vae.config.scaling_factor * latents\n",
    "    image = vae.decode(latents).sample\n",
    "    return image\n",
    "\n",
    "# TODO: Set up gradient scaler for mixed precision training (optional)\n",
    "scaler = torch.amp.GradScaler(\"cuda\", init_scale=2**16, growth_interval=1000, growth_factor=2.0)\n",
    "# 这里我们使用 GradScaler 来支持混合精度训练，这可以提高训练速度并减少内存使用。\n",
    "# 但是注意，我们下面的训练循环中会需要使用 autocast 来自动处理混合精度。\n",
    "\n",
    "# TODO: Initialize training tracking variables\n",
    "losses = []\n",
    "print(\"Training setup complete. Optimizer, noise scheduler, helper functions, and GradScaler are ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d14f9f9",
   "metadata": {},
   "source": [
    "`Gradient Scaler` **(梯度缩放器) 是一个用来解决混合精度训练中“数值下溢” (Underflow) 问题的工具，它能保证模型在低精度浮点数（如FP16）下依然可以稳定地训练**。\n",
    "\n",
    "1. **什么是“混合精度训练” (Mixed Precision Training)？**\n",
    "\n",
    "    在深度学习中，我们通常使用32位浮点数（FP32，或称单精度）来存储和计算模型的权重、梯度等数据。\n",
    "\n",
    "    混合精度训练则是指在训练过程中，同时使用16位浮点数（FP16，或称半精度）和32位浮点数（FP32）。\n",
    "\n",
    "    优点：\n",
    "\n",
    "    * **速度更快**：现代的GPU（尤其是NVIDIA带有Tensor Cores的显卡）计算FP16的速度远超FP32。\n",
    "\n",
    "    * **显存更少**：FP16占用的存储空间是FP32的一半，这意味着你可以用更大的模型、更大的批量（batch size）进行训练。\n",
    "\n",
    "    缺点/挑战：\n",
    "\n",
    "    * **数值范围更小**：FP16能表示的数值范围比FP32小得多。如果一个数非常小，超出了FP16的表示范围，它就会被当作**零**，这种情况称为“**数值下溢**”(Numerical Underflow)。\n",
    "\n",
    "2. **为什么“数值下溢”是个大问题？**\n",
    "\n",
    "    在模型训练的反向传播（backpropagation）过程中，计算出的梯度 (gradients) 可能非常小。如果使用FP16，这些微小的梯度值很容易因为“数值下溢”而变成零。\n",
    "\n",
    "    一旦梯度变成零，模型就无法从这些信息中学到任何东西，参数也就不会被更新。这就好比老师给学生划重点，但声音太小，学生一个字也听不见，自然也就无法学习和进步。最终，这会导致模型**无法收敛或训练失败**。\n",
    "\n",
    "3. `Gradient Scaler` **如何解决这个问题？**\n",
    "    \n",
    "    `Gradient Scaler` (在PyTorch中是 `torch.cuda.amp.GradScaler`) 通过一个巧妙的“放大再缩小”的策略来解决问题：\n",
    "\n",
    "    工作流程如下：\n",
    "\n",
    "    1. **放大损失 (Loss Scaling)**：在计算反向传播之前，GradScaler 会将计算出的损失值（Loss）乘以一个巨大的缩放因子（例如 65536.0）。\n",
    "\n",
    "    2. **计算放大的梯度 (Scaled Backward Pass)**：当使用这个被放大了的损失值进行反向传播时，根据链式法则，所有计算出的梯度也会被同样地放大。\n",
    "\n",
    "        这样一来，那些原本非常微小的、可能会下溢的梯度，现在被“抬升”到了FP16可以安全表示的范围内，避免了信息丢失。\n",
    "\n",
    "    3. **缩小梯度 (Unscaling)**：梯度虽然安全了，但它们是“虚高”的，不能直接用来更新模型权重。因此，在优化器（Optimizer）更新权重之前，`GradScaler` 会将这些被放大的梯度**除以同一个缩放因子**，将它们恢复到原始的、正确的数值。\n",
    "\n",
    "    4. **动态调整缩放因子 (Dynamic Scaling)：** `GradScaler`非常智能，它会动态调整这个缩放因子。\n",
    "\n",
    "        如果梯度在放大后出现了无穷大（`inf`）或非数字（`NaN`）（说明缩放因子太大，导致“数值上溢”了），`GradScaler` 会跳过这次的参数更新，并在下一次迭代时减小缩放因子。\n",
    "\n",
    "        如果连续很多次迭代都没有出现问题，它会尝试增大缩放因子，以利用FP16更广的动态范围。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f156d0",
   "metadata": {},
   "source": [
    "`@torch.no_grad()` 是一个 PyTorch 中的**装饰器 (decorator)**，它的作用非常明确和重要：**在该装饰器下的函数运行时，临时禁用梯度计算**。\n",
    "\n",
    "简单来说，当代码被 `@torch.no_grad()` 包裹时，PyTorch 就不会去追踪和记录任何张量（Tensor）的操作历史，因此也就无法为这些操作计算梯度。\n",
    "\n",
    "这会带来两个核心的好处：\n",
    "\n",
    "1. **节省显存/内存**：因为不需要存储计算图（computation graph）和中间状态来为反向传播做准备，所以会显著减少内存的消耗。\n",
    "\n",
    "2. **加快计算速度**：由于省去了追踪操作的开销，代码的执行速度会更快。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2589ce",
   "metadata": {},
   "source": [
    "## 6️⃣ Training Loop Implementation\n",
    "\n",
    "**Task**: Implement the main training loop for diffusion model finetuning.\n",
    "\n",
    "**Requirements**:\n",
    "- Encode images to latent space using VAE\n",
    "- Add noise to latents according to diffusion schedule\n",
    "- Predict noise using UNet conditioned on text\n",
    "- Compute loss between predicted and actual noise\n",
    "- Update UNet parameters via backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3b9e5c",
   "metadata": {},
   "source": [
    "在Stable Diffusion的训练过程中，我们使用 **MSE Loss (Mean Squared Error, 均方误差损失)**，是因为这个任务的本质是一个**回归问题**，而MSE是解决这类问题的经典且高效的工具。\n",
    "\n",
    "具体来说，我们训练的目标是**让模型精确地预测出我们当初添加的噪声**。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fcf29e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training with mixed precision...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0eebfdcf24544455a1c445f6061b3b28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/1221 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rh/2t3sy5bx13nctf2n07gkzskh0000gn/T/ipykernel_2187/1205080145.py:18: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(\"mps\"):\n",
      "/opt/anaconda3/lib/python3.12/site-packages/torch/amp/autocast_mode.py:266: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Cannot convert a MPS Tensor to float64 dtype as the MPS framework doesn't support float64. Please use float32 instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 42\u001b[0m\n\u001b[1;32m     40\u001b[0m scaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# 使用 scaler 来执行优化器步骤\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m scaler\u001b[38;5;241m.\u001b[39mstep(optimizer)\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# 更新 scaler 为下一次迭代做准备\u001b[39;00m\n\u001b[1;32m     44\u001b[0m scaler\u001b[38;5;241m.\u001b[39mupdate()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/amp/grad_scaler.py:455\u001b[0m, in \u001b[0;36mGradScaler.step\u001b[0;34m(self, optimizer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    452\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m retval\n\u001b[1;32m    454\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstage\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m OptState\u001b[38;5;241m.\u001b[39mREADY:\n\u001b[0;32m--> 455\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munscale_(optimizer)\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[1;32m    458\u001b[0m     \u001b[38;5;28mlen\u001b[39m(optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    459\u001b[0m ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo inf checks were recorded for this optimizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    461\u001b[0m retval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_opt_step(optimizer, optimizer_state, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/amp/grad_scaler.py:339\u001b[0m, in \u001b[0;36mGradScaler.unscale_\u001b[0;34m(self, optimizer)\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[38;5;66;03m# FP32 division can be imprecise for certain compile options, so we carry out the reciprocal in FP64.\u001b[39;00m\n\u001b[1;32m    338\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_scale \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 339\u001b[0m inv_scale \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_scale\u001b[38;5;241m.\u001b[39mdouble()\u001b[38;5;241m.\u001b[39mreciprocal()\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m    340\u001b[0m found_inf \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfull((), \u001b[38;5;241m0.0\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_scale\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    342\u001b[0m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_unscale_grads_(\n\u001b[1;32m    343\u001b[0m     optimizer, inv_scale, found_inf, \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    344\u001b[0m )\n",
      "\u001b[0;31mTypeError\u001b[0m: Cannot convert a MPS Tensor to float64 dtype as the MPS framework doesn't support float64. Please use float32 instead."
     ]
    }
   ],
   "source": [
    "# TODO: Implement training loop:\n",
    "print(\"Starting training with mixed precision...\")\n",
    "unet.train()  # Set UNet to training mode\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    progress_bar = tqdm(total=len(train_dataloader), desc=f\"Epoch {epoch + 1}\")\n",
    "    \n",
    "    #       For each batch in train_dataloader:\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        #           - Get images and text from batch\n",
    "        pixel_values = batch[\"pixel_values\"] \n",
    "        input_ids = batch[\"input_ids\"]\n",
    "\n",
    "        # Clear gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 使用 autocast 上下文管理器进行混合精度前向传播\n",
    "        with autocast(\"cuda\"):\n",
    "            #           - Encode images to latent space using VAE\n",
    "            latents = encode_image(pixel_values)\n",
    "\n",
    "            #           - Encode text to embeddings using text encoder\n",
    "            encoder_hidden_states = encode_text(input_ids)\n",
    "            \n",
    "            #           - Sample random timesteps for diffusion\n",
    "            noise = torch.randn_like(latents) # 生成一个与latents张量形状完全相同，但填充了标准正态分布（高斯噪声）随机数的噪声张量。\n",
    "            timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (latents.shape[0],), device=latents.device).long()\n",
    "            \n",
    "            #           - Add noise to latents according to schedule\n",
    "            noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n",
    "            \n",
    "            #           - Predict noise using UNet with text conditioning\n",
    "            noise_pred = unet(noisy_latents, timesteps, encoder_hidden_states, return_dict=False)[0]\n",
    "\n",
    "            #           - Compute MSE loss between predicted and actual noise\n",
    "            loss = F.mse_loss(noise_pred, noise)\n",
    "        \n",
    "        #           - Backpropagate and update UNet parameters using the GradScaler\n",
    "        # 使用 scaler 来缩放损失并进行反向传播\n",
    "        scaler.scale(loss).backward()\n",
    "        # 使用 scaler 来执行优化器步骤\n",
    "        scaler.step(optimizer)\n",
    "        # 更新 scaler 为下一次迭代做准备\n",
    "        scaler.update()\n",
    "        \n",
    "        #           - Track and display training progress\n",
    "        losses.append(loss.item())\n",
    "        progress_bar.set_postfix(loss=loss.item())\n",
    "        progress_bar.update(1)\n",
    "\n",
    "    progress_bar.close()\n",
    "\n",
    "# TODO: Save model checkpoints periodically\n",
    "# unet.save_pretrained(\"naruto_finetuned_unet_mixed_precision\")\n",
    "print(\"Training finished.\")\n",
    "\n",
    "# TODO: Display loss curves and training statistics\n",
    "# This will be done in the evaluation section (Block 9)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d398bffa",
   "metadata": {},
   "source": [
    "## 7️⃣ Inference Pipeline Setup\n",
    "\n",
    "**Task**: Create inference pipeline for text-to-image generation.\n",
    "\n",
    "**Requirements**:\n",
    "- Set up complete diffusion pipeline with trained UNet\n",
    "- Configure scheduler for inference (100 steps)\n",
    "- Implement text-to-image generation function\n",
    "- Handle classifier-free guidance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f5c4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create inference pipeline:\n",
    "#       - Set all models to eval mode\n",
    "#       - Create StableDiffusionPipeline with trained components\n",
    "#       - Configure scheduler for inference\n",
    "# 导入 StableDiffusionPipeline 类\n",
    "from diffusers import StableDiffusionPipeline\n",
    "\n",
    "#       - Set all models to eval mode\n",
    "#       - Create StableDiffusionPipeline with trained components\n",
    "# 我们将所有微调过的（unet）和未改动的（vae, text_encoder等）组件加载到一个Pipeline中\n",
    "# 这是进行推理的推荐方法，代码更简洁且不易出错。\n",
    "pipeline = StableDiffusionPipeline(\n",
    "    vae=vae,\n",
    "    text_encoder=text_encoder,\n",
    "    tokenizer=tokenizer,\n",
    "    unet=unet,\n",
    "    scheduler=scheduler,\n",
    "    safety_checker=None, # 小型模型通常没有安全检查器\n",
    "    feature_extractor=None,\n",
    "    requires_safety_checker=False,\n",
    ")\n",
    "# StableDiffusionPipeline 内部会自动处理 Set all models to eval mode！\n",
    "\n",
    "# 将整个 pipeline 移动到指定设备\n",
    "pipeline.to(device)\n",
    "print(\"StableDiffusionPipeline created and moved to device.\")\n",
    "\n",
    "#       - Configure scheduler for inference\n",
    "# Pipeline 会自动处理推理时 scheduler 的配置，我们只需在调用时传入步数。\n",
    "\n",
    "# TODO: Implement generate_image function that:\n",
    "#       - Takes text prompt as input\n",
    "#       - Encodes text to embeddings (handled by pipeline)\n",
    "#       - Starts with random noise (handled by pipeline)\n",
    "#       - Performs denoising for specified number of steps (handled by pipeline)\n",
    "#       - Decodes final latent to image (handled by pipeline)\n",
    "#       - Returns PIL image\n",
    "@torch.no_grad()\n",
    "def generate_image(prompt, num_inference_steps, guidance_scale, seed=None):\n",
    "    # 使用生成器以确保在使用种子时结果可复现\n",
    "    generator = torch.manual_seed(seed) if seed is not None else None\n",
    "    \n",
    "    # 调用 pipeline 生成图像\n",
    "    # pipeline 会自动处理文本编码、CFG、降噪循环和VAE解码\n",
    "    result = pipeline(\n",
    "        prompt=prompt,\n",
    "        num_inference_steps=num_inference_steps,\n",
    "        guidance_scale=guidance_scale,\n",
    "        generator=generator\n",
    "    )\n",
    "    \n",
    "    # 从结果中获取图像\n",
    "    image = result.images[0]\n",
    "    return image\n",
    "\n",
    "# TODO: Set up proper inference configuration:\n",
    "#       - num_inference_steps = INFERENCE_STEPS\n",
    "#       - guidance_scale = GUIDANCE_SCALE\n",
    "#       - Enable safety checker if desired (already disabled in pipeline)\n",
    "print(f\"Inference function 'generate_image' is ready.\")\n",
    "print(f\"Default inference steps: {INFERENCE_STEPS}, Guidance scale: {GUIDANCE_SCALE}\")\n",
    "# 在最开始的Define configuration parameters部分已经定义了 INFERENCE_STEPS 和 GUIDANCE_SCALE。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b2289b",
   "metadata": {},
   "source": [
    "**Pipeline (管道)**\n",
    "\n",
    "在Hugging Face `diffusers` 库的语境下，`Pipeline` (管道) 是一个**高级、一体化的工具，它将运行一个复杂模型（如Stable Diffusion）所需的所有组件和步骤都封装打包在了一起**。\n",
    "\n",
    "你可以把它想象成一个“**全自动的图像生成工厂**”。你不需要关心工厂内部的各个车间（模型组件）是如何协同工作的，你只需要向工厂下订单（提供一个文本提示 `prompt`），工厂就能自动完成所有工序，最终交付给你成品（一张图片）。\n",
    "\n",
    "一个典型的 `StableDiffusionPipeline` 包含以下这些核心组件：\n",
    "\n",
    "1. **Tokenizer (分词器)**：负责接收你输入的文本（如 \"a photo of an astronaut riding a horse on mars\"），并将其转换成模型能理解的数字ID。\n",
    "\n",
    "2. **Text Encoder (文本编码器)**：接收分词器输出的数字ID，并将其转换成包含丰富语义信息的向量（embeddings）。这是模型理解你“想要什么”的关键。\n",
    "\n",
    "3. **UNet (核心降噪模型)**：这是扩散模型的心脏。它接收一个随机噪声图和文本编码器的输出，然后在多个步骤中，逐步去除噪声，最终“雕刻”出符合文本描述的图像的潜空间表示。你在微调（finetuning）时，训练的就是这个组件。\n",
    "\n",
    "4. **VAE (Variational Autoencoder, 变分自编码器)**：\n",
    "\n",
    "    编码：在训练开始前，它负责将原始图片压缩到更小、更高效的潜空间（`latents`）。\n",
    "\n",
    "    解码：在推理结束时，它负责将UNet生成的、干净的潜空间表示解码（“放大”）回我们能看到的正常像素图像。\n",
    "\n",
    "5. **Scheduler (调度器)**：负责管理整个降噪过程的“步调”。它定义了总共有多少个降噪步骤，以及在每个步骤中应该去除多少噪声。不同的调度器（如 `PNDM`, `DPM-Solver`）有不同的去噪策略，会影响生成速度和图片质量。\n",
    "\n",
    "**Pipeline 的核心优点：**\n",
    "\n",
    "* **便捷性**：它隐藏了所有复杂的内部调用流程。你只需要一行代码 `pipeline(prompt)` 就能生成图片，而不需要手动调用5个不同的组件并传递数据。\n",
    "\n",
    "* **一致性与正确性**：它能确保所有组件都以正确的方式（例如，在推理时自动设置 `.eval()` 模式）协同工作，减少了用户出错的可能性。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4bea33",
   "metadata": {},
   "source": [
    "## 8️⃣ Generate Images with Dataset Prompts\n",
    "\n",
    "**Task**: Generate images using 5 prompts from the training dataset.\n",
    "\n",
    "**Requirements**:\n",
    "- Select 5 different text prompts from the dataset\n",
    "- Generate images for each prompt\n",
    "- Display results in a grid format\n",
    "- Show prompt text alongside generated images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95f99e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Select 5 prompts from training dataset:\n",
    "#       - Use different indices to get variety\n",
    "#       - Extract text descriptions\n",
    "dataset_prompts = [ds['train'][i]['text'] for i in [10, 100, 200, 300, 400]]\n",
    "\n",
    "#       - Set random seed for reproducibility\n",
    "generation_seed = 1337\n",
    "generated_images_dataset = []\n",
    "\n",
    "# TODO: Generate images for each dataset prompt:\n",
    "for prompt in dataset_prompts:\n",
    "    print(f\"Generating image for prompt: '{prompt}'\")\n",
    "    #       - Use generate_image function\n",
    "    #       - Set random seed for reproducibility\n",
    "    image = generate_image(\n",
    "        prompt,\n",
    "        num_inference_steps=INFERENCE_STEPS,\n",
    "        guidance_scale=GUIDANCE_SCALE,\n",
    "        seed=generation_seed\n",
    "    )\n",
    "    #       - Save generated images\n",
    "    generated_images_dataset.append(image)\n",
    "\n",
    "# TODO: Create visualization:\n",
    "#       - Display each prompt text\n",
    "#       - Show corresponding generated image\n",
    "#       - Use matplotlib subplot for clean layout\n",
    "#       - Add titles and proper formatting\n",
    "fig, axes = plt.subplots(1, 5, figsize=(25, 5))\n",
    "for i, (prompt, img) in enumerate(zip(dataset_prompts, generated_images_dataset)):\n",
    "    axes[i].imshow(img)\n",
    "    axes[i].set_title(f\"Prompt: \\\"{prompt[:40]}...\\\"\", fontsize=10)\n",
    "    axes[i].axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.suptitle(\"Generated Images from Training Dataset Prompts\", fontsize=16)\n",
    "\n",
    "# TODO: Display results in a 2x3 grid or similar arrangement\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecec78db",
   "metadata": {},
   "source": [
    "`guidance_scale` controls how strictly the model follows your text prompt.\n",
    "\n",
    "The `seed` (or random seed) is a number that initializes the model's random number generator."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ffe12b7",
   "metadata": {},
   "source": [
    "## 9️⃣ Generate Images with Custom Prompts\n",
    "\n",
    "**Task**: Generate images using 5 custom prompts that you create.\n",
    "\n",
    "**Requirements**:\n",
    "- Write 5 creative prompts in Naruto style\n",
    "- Test different types of descriptions (characters, scenes, actions)\n",
    "- Generate and display results\n",
    "- Compare quality with dataset prompt results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ef5352",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define 5 custom prompts, for example:\n",
    "custom_prompts = [\n",
    "    \"A ninja with sharingan eyes, wearing a black cloak, standing in the rain, naruto style\",\n",
    "    \"A majestic nine-tailed fox spirit with glowing red chakra, naruto style\",\n",
    "    \"A beautiful kunoichi with pink hair, sad expression, cherry blossoms falling around her, naruto style\",\n",
    "    \"An epic battle between two powerful shinobis on top of a giant statue, naruto style\",\n",
    "    \"The hidden leaf village seen from the hokage rock at night, naruto style\"\n",
    "]\n",
    "\n",
    "#       - Use same generation parameters as before\n",
    "#       - Ensure consistent quality\n",
    "generation_seed = 42\n",
    "generated_images_custom = []\n",
    "\n",
    "# TODO: Generate images for each custom prompt:\n",
    "for prompt in custom_prompts:\n",
    "    print(f\"Generating image for custom prompt: '{prompt}'\")\n",
    "    image = generate_image(\n",
    "        prompt,\n",
    "        num_inference_steps=INFERENCE_STEPS,\n",
    "        guidance_scale=GUIDANCE_SCALE,\n",
    "        seed=generation_seed\n",
    "    )\n",
    "    generated_images_custom.append(image)\n",
    "\n",
    "# TODO: Create visualization for custom prompts:\n",
    "#       - Similar layout to dataset prompts\n",
    "#       - Show prompt text and generated image\n",
    "fig, axes = plt.subplots(1, 5, figsize=(25, 5))\n",
    "for i, (prompt, img) in enumerate(zip(custom_prompts, generated_images_custom)):\n",
    "    axes[i].imshow(img)\n",
    "    axes[i].set_title(f\"Prompt: \\\"{prompt[:40]}...\\\"\", fontsize=10)\n",
    "    axes[i].axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.suptitle(\"Generated Images from Custom Prompts\", fontsize=16)\n",
    "\n",
    "# TODO: Display all 5 custom prompt results\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c68346",
   "metadata": {},
   "source": [
    "## 🔟 Model Evaluation and Comparison\n",
    "\n",
    "**Task**: Evaluate and compare your results\n",
    "\n",
    "**Requirements**:\n",
    "- Compare generated images with original dataset images\n",
    "- Evaluate image quality, style consistency, and prompt adherence\n",
    "- Plot training progress and loss convergence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfcc58ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create comparison visualization:\n",
    "#       - Show original dataset images alongside generated ones\n",
    "#       - Compare style consistency\n",
    "#       - Evaluate prompt adherence\n",
    "print(\"Comparing original dataset images with generated images.\")\n",
    "indices_to_compare = [50, 150, 250, 350, 450]\n",
    "prompts_to_compare = [ds['train'][i]['text'] for i in indices_to_compare]\n",
    "original_images = [ds['train'][i]['image'].convert(\"RGB\") for i in indices_to_compare]\n",
    "generated_images_compare = []\n",
    "\n",
    "for prompt in prompts_to_compare:\n",
    "    print(f\"Generating for comparison: '{prompt}'\")\n",
    "    img = generate_image(prompt, num_inference_steps=INFERENCE_STEPS, guidance_scale=GUIDANCE_SCALE, seed=8888)\n",
    "    generated_images_compare.append(img)\n",
    "\n",
    "fig, axes = plt.subplots(2, 5, figsize=(25, 10))\n",
    "fig.suptitle('Original vs. Generated Image Comparison', fontsize=16)\n",
    "\n",
    "for i in range(5):\n",
    "    # Plot original images\n",
    "    axes[0, i].imshow(original_images[i])\n",
    "    axes[0, i].set_title(f\"Original: \\\"{prompts_to_compare[i][:30]}...\\\"\", fontsize=10)\n",
    "    axes[0, i].axis('off')\n",
    "    \n",
    "    # Plot generated images\n",
    "    axes[1, i].imshow(generated_images_compare[i])\n",
    "    axes[1, i].set_title(f\"Generated: \\\"{prompts_to_compare[i][:30]}...\\\"\", fontsize=10)\n",
    "    axes[1, i].axis('off')\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# TODO: Plot training loss curve:\n",
    "#       - Show loss progression over epochs\n",
    "#       - Analyze convergence behavior\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(losses)\n",
    "plt.title(\"Training Loss Convergence\")\n",
    "plt.xlabel(\"Training Steps\")\n",
    "plt.ylabel(\"MSE Loss\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(\"Evaluation complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936c8434",
   "metadata": {},
   "source": [
    "## 📝 Evaluation Criteria\n",
    "\n",
    "Your homework will be evaluated based on:\n",
    "\n",
    "1. **Implementation Correctness (40%)**\n",
    "   - Proper stable diffusion pipeline setup\n",
    "   - Correct training loop implementation\n",
    "   - Working inference pipeline\n",
    "   - Appropriate use of VAE, UNet, text encoder, and scheduler\n",
    "\n",
    "2. **Training and Results (30%)**\n",
    "   - Model trains without errors\n",
    "   - Reasonable loss convergence\n",
    "   - Generated images show Naruto style characteristics\n",
    "   - Successful generation from both dataset and custom prompts\n",
    "\n",
    "3. **Code Quality (30%)**\n",
    "   - Clean, readable code with proper comments\n",
    "   - Efficient memory usage and error handling\n",
    "   - Proper tensor operations and device management\n",
    "   - Good visualization and presentation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
