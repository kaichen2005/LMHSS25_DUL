{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](https://i.imgur.com/a3uAqnb.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contents\n",
    "\n",
    "In this notebook, we will create a RealNVP(real-valued non-volume preserving) generative model for the Oxford Pets dataset, with an intermediate AutoEncoder.\n",
    "\n",
    "Instead of training the model on direct pixel values and generating images, we will\n",
    "\n",
    "1. Train an AutoEncoder for the images\n",
    "2. Convert the data into embeddings using the AutoEncoder\n",
    "3. Train our RealNVP model on the embeddings\n",
    "4. Generate embeddings using the RealNVP model and convert them to images using the AutoEncoder's decoder\n",
    "\n",
    "This notebook is heavily based on [This Repo](https://github.com/SpencerSzabados/realnvp-pytorch/tree/master)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "from torch import nn, distributions\n",
    "from torch.nn import MSELoss\n",
    "from torchvision import transforms\n",
    "from datasets import load_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearBatchNorm(nn.Module):\n",
    "    \"\"\"\n",
    "    An (invertible) batch normalization layer.\n",
    "    This class is mostly inspired from this one:\n",
    "    https://github.com/kamenbliznashki/normalizing_flows/blob/master/maf.py\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size, momentum=0.9, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.momentum = momentum\n",
    "        self.eps = eps\n",
    "\n",
    "        self.log_gamma = nn.Parameter(torch.zeros(input_size))\n",
    "        self.beta = nn.Parameter(torch.zeros(input_size))\n",
    "\n",
    "        self.register_buffer('running_mean', torch.zeros(input_size))\n",
    "        self.register_buffer('running_var', torch.ones(input_size))\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        if self.training:\n",
    "            self.batch_mean = x.mean(0)\n",
    "            self.batch_var = x.var(0)\n",
    "\n",
    "            self.running_mean.mul_(self.momentum).add_(self.batch_mean.data * (1 - self.momentum))\n",
    "            self.running_var.mul_(self.momentum).add_(self.batch_var.data * (1 - self.momentum))\n",
    "\n",
    "            mean = self.batch_mean\n",
    "            var = self.batch_var\n",
    "        else:\n",
    "            mean = self.running_mean\n",
    "            var = self.running_var\n",
    "\n",
    "        x_hat = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        y = self.log_gamma.exp() * x_hat + self.beta\n",
    "\n",
    "        log_det = self.log_gamma - 0.5 * torch.log(var + self.eps)\n",
    "\n",
    "        return y, log_det.expand_as(x).sum(1)\n",
    "\n",
    "    def backward(self, x, **kwargs):\n",
    "        if self.training:\n",
    "            mean = self.batch_mean\n",
    "            var = self.batch_var\n",
    "        else:\n",
    "            mean = self.running_mean\n",
    "            var = self.running_var\n",
    "\n",
    "        x_hat = (x - self.beta) * torch.exp(-self.log_gamma)\n",
    "        x = x_hat * torch.sqrt(var + self.eps) + mean\n",
    "\n",
    "        log_det = 0.5 * torch.log(var + self.eps) - self.log_gamma\n",
    "\n",
    "        return x, log_det.expand_as(x).sum(1)\n",
    "\n",
    "\n",
    "class LinearCouplingLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Linear coupling layer.\n",
    "        (i) Split the input x into 2 parts x1 and x2 according to a given mask.\n",
    "        (ii) Compute s(x2) and t(x2) with given neural network.\n",
    "        (iii) Final output is [exp(s(x2))*x1 + t(x2); x2].\n",
    "    The inverse is trivially [(x1 - t(x2))*exp(-s(x2)); x2].\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, mask, network_topology, conditioning_size=None, single_function=True):\n",
    "        super().__init__()\n",
    "\n",
    "        if conditioning_size is None:\n",
    "            conditioning_size = 0\n",
    "\n",
    "        if network_topology is None or len(network_topology) == 0:\n",
    "            network_topology = [input_dim]\n",
    "\n",
    "        self.register_buffer('mask', mask)\n",
    "\n",
    "        self.dim = input_dim\n",
    "\n",
    "        self.s = [nn.Linear(input_dim + conditioning_size, network_topology[0]), nn.ReLU()]\n",
    "\n",
    "        for i in range(len(network_topology)):\n",
    "            t = network_topology[i]\n",
    "            t_p = network_topology[i - 1]\n",
    "            self.s.extend([nn.Linear(t_p, t), nn.ReLU()])\n",
    "\n",
    "        if single_function:\n",
    "            input_dim = input_dim * 2\n",
    "\n",
    "        ll = nn.Linear(network_topology[-1], input_dim)\n",
    "\n",
    "        self.s.append(ll)\n",
    "        self.s = nn.Sequential(*self.s)\n",
    "\n",
    "        if single_function:\n",
    "            self.st = lambda x: (self.s(x).chunk(2, 1))\n",
    "        else:\n",
    "            self.t = copy.deepcopy(self.s)\n",
    "            self.st = lambda x: (self.s(x), self.t(x))\n",
    "\n",
    "    def backward(self, x, y=None):\n",
    "        mx = x * self.mask\n",
    "\n",
    "        if y is not None:\n",
    "            _mx = torch.cat([y, mx], dim=1)\n",
    "        else:\n",
    "            _mx = mx\n",
    "\n",
    "        s, t = self.st(_mx)\n",
    "        s = torch.tanh(s)\n",
    "\n",
    "        u = mx + (1 - self.mask) * (x - t) * torch.exp(-s)\n",
    "\n",
    "        log_abs_det_jacobian = - (1 - self.mask) * s\n",
    "\n",
    "        return u, log_abs_det_jacobian.sum(1)\n",
    "\n",
    "    def forward(self, u, y=None):\n",
    "        mu = u * self.mask\n",
    "\n",
    "        if y is not None:\n",
    "            _mu = torch.cat([y, mu], dim=1)\n",
    "        else:\n",
    "            _mu = mu\n",
    "\n",
    "        s, t = self.st(_mu)\n",
    "        s = torch.tanh(s)\n",
    "\n",
    "        x = mu + (1 - self.mask) * (u * s.exp() + t)\n",
    "\n",
    "        log_abs_det_jacobian = (1 - self.mask) * s\n",
    "\n",
    "        return x, log_abs_det_jacobian.sum(1)\n",
    "\n",
    "\n",
    "class Permutation(nn.Module):\n",
    "    \"\"\"\n",
    "    A permutation layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_ch):\n",
    "        super().__init__()\n",
    "        self.in_ch = in_ch\n",
    "        self.register_buffer('p', torch.randperm(in_ch))\n",
    "        self.register_buffer('invp', torch.argsort(self.p))\n",
    "\n",
    "    def forward(self, x, y=None):\n",
    "        assert x.shape[1] == self.in_ch\n",
    "        out = x[:, self.p]\n",
    "        return out, 0\n",
    "\n",
    "    def backward(self, x, y=None):\n",
    "        assert x.shape[1] == self.in_ch\n",
    "        out = x[:, self.invp]\n",
    "        return out, 0\n",
    "\n",
    "\n",
    "class SequentialFlow(nn.Sequential):\n",
    "    \"\"\"\n",
    "    Utility class to build a normalizing flow from a sequence of base transformations.\n",
    "    During forward and inverse steps, aggregates the sum of the log determinants of the Jacobians.\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, x, y=None):\n",
    "        log_det = 0\n",
    "        for module in self:\n",
    "            x, _log_det = module(x, y=y)\n",
    "            log_det = log_det + _log_det\n",
    "        return x, log_det\n",
    "\n",
    "    def backward(self, u, y=None):\n",
    "        log_det = 0\n",
    "        for module in reversed(self):\n",
    "            u, _log_det = module.backward(u, y=y)\n",
    "            log_det = log_det + _log_det\n",
    "        return u, log_det\n",
    "\n",
    "    def forward_steps(self, x, y=None):\n",
    "        log_det = 0\n",
    "        xs = [x]\n",
    "        for module in self:\n",
    "            x, _log_det = module(x, y=y)\n",
    "            xs.append(x)\n",
    "            log_det = log_det + _log_det\n",
    "        return xs, log_det\n",
    "\n",
    "    def backward_steps(self, u, y=None):\n",
    "        log_det = 0\n",
    "        us = [u]\n",
    "        for module in reversed(self):\n",
    "            u, _log_det = module.backward(u, y=y)\n",
    "            us.append(u)\n",
    "            log_det = log_det + _log_det\n",
    "        return us, log_det\n",
    "\n",
    "\n",
    "class LinearRNVP(nn.Module):\n",
    "    \"\"\"\n",
    "    Main RNVP model, alternating affine coupling layers\n",
    "    with permutations and/or batch normalization steps.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, coupling_topology, flow_n=2, use_permutation=False,\n",
    "                 batch_norm=False, mask_type='odds', conditioning_size=None, single_function=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.register_buffer('prior_mean', torch.zeros(input_dim))\n",
    "        self.register_buffer('prior_var', torch.ones(input_dim))\n",
    "\n",
    "        if mask_type == 'odds':\n",
    "            mask = torch.arange(0, input_dim).float() % 2\n",
    "        elif mask_type == 'half':\n",
    "            mask = torch.zeros(input_dim)\n",
    "            mask[:input_dim // 2] = 1\n",
    "        else:\n",
    "            assert False\n",
    "\n",
    "        if coupling_topology is None:\n",
    "            coupling_topology = [input_dim // 2, input_dim // 2]\n",
    "\n",
    "        blocks = []\n",
    "\n",
    "        for i in range(flow_n):\n",
    "\n",
    "            blocks.append(LinearCouplingLayer(input_dim, mask, network_topology=coupling_topology,\n",
    "                                              conditioning_size=conditioning_size, single_function=single_function))\n",
    "\n",
    "            if use_permutation:\n",
    "                blocks.append(Permutation(input_dim))\n",
    "            else:\n",
    "                mask = 1 - mask\n",
    "\n",
    "            if batch_norm:\n",
    "                blocks.append(LinearBatchNorm(input_dim))\n",
    "\n",
    "        self.flows = SequentialFlow(*blocks)\n",
    "\n",
    "    def logprob(self, x):\n",
    "        return self.prior.log_prob(x)\n",
    "\n",
    "    @property\n",
    "    def prior(self):\n",
    "        return distributions.Normal(self.prior_mean, self.prior_var)\n",
    "\n",
    "    def forward(self, x, y=None, return_step=False):\n",
    "        if return_step:\n",
    "            return self.flows.forward_steps(x, y)\n",
    "        return self.flows.forward(x, y)\n",
    "\n",
    "    def backward(self, u, y=None, return_step=False):\n",
    "        if return_step:\n",
    "            return self.flows.backward_steps(u, y)\n",
    "        return self.flows.backward(u, y)\n",
    "\n",
    "    def sample(self, samples=1, y=None, return_step=False, return_logdet=False):\n",
    "        u = self.prior.sample((samples,))\n",
    "        z, d = self.backward(u, y=y, return_step=return_step)\n",
    "        if return_logdet:\n",
    "            d = self.logprob(u).sum(1) + d\n",
    "            return z, d\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the random seeds\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# Load the Oxford Pets dataset from Hugging Face\n",
    "dataset = load_dataset(\"enterprise-explorers/oxford-pets\")\n",
    "train_dataset = dataset['train']\n",
    "\n",
    "\n",
    "# Create a custom dataset class for the Oxford Pets data\n",
    "class OxfordPetsDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, hf_dataset, transform=None):\n",
    "        self.dataset = hf_dataset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        image = item['image']\n",
    "\n",
    "        # Convert to RGB if not already\n",
    "        if image.mode != 'RGB':\n",
    "            image = image.convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Use the 'dog' field: True for dog (1), False for cat (0)\n",
    "        label = 1 if item['dog'] else 0\n",
    "\n",
    "        return image, label\n",
    "\n",
    "\n",
    "train_set = OxfordPetsDataset(train_dataset, transform=transform)\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_set, BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoencoder definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **📌 Autoencoder for Compressing Pet Images**\n",
    "\n",
    "The Oxford Pets dataset contains much more complex images (64x64, RGB, varied poses and backgrounds) than MNIST. To handle this, we use a more powerful **Autoencoder** with a deeper, DCGAN-style architecture.\n",
    "\n",
    "## **🔹 How it Works**\n",
    "\n",
    "1️⃣ **Encoder**: A deep convolutional network that compresses a 64x64 pet image into a low-dimensional latent vector (embedding). This embedding must capture the key features that define the pet's appearance and species.\n",
    "\n",
    "2️⃣ **Decoder**: A deconvolutional network that attempts to perfectly reconstruct the original image from this compressed embedding.\n",
    "\n",
    "By training this model to minimize reconstruction error, we create a rich, low-dimensional \"embedding space\" that we can then model with our Normalizing Flow.\n",
    "\n",
    "\n",
    "\n",
    "## **📌 Expected Input & Output Shapes**\n",
    "\n",
    "- **Input Image:** `(batch_size, 3, 64, 64)`\n",
    "- **Latent Embedding:** `(batch_size, 256)`  *(256 is our `EMBEDDING_DIM`)*\n",
    "- **Reconstructed Image:** `(batch_size, 3, 64, 64)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImprovedAutoEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            # 64x64x3 -> 32x32x64\n",
    "            nn.Conv2d(3, 64, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            # 32x32x64 -> 16x16x128\n",
    "            nn.Conv2d(64, 128, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            # 16x16x128 -> 8x8x256\n",
    "            nn.Conv2d(128, 256, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            # 8x8x256 -> 4x4x512\n",
    "            nn.Conv2d(256, 512, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            # 4x4x512 -> 2x2x512\n",
    "            nn.Conv2d(512, 512, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "        )\n",
    "\n",
    "        # Bottleneck layers\n",
    "        self.flatten_size = 512 * 2 * 2  # 2048\n",
    "        self.fc_encode = nn.Sequential(\n",
    "            nn.Linear(self.flatten_size, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(1024, EMBEDDING_DIM)\n",
    "        )\n",
    "\n",
    "        self.fc_decode = nn.Sequential(\n",
    "            nn.Linear(EMBEDDING_DIM, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(1024, self.flatten_size),\n",
    "            nn.BatchNorm1d(self.flatten_size),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        # Decoder with symmetric architecture\n",
    "        self.decoder = nn.Sequential(\n",
    "            # 2x2x512 -> 4x4x512\n",
    "            nn.ConvTranspose2d(512, 512, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            # 4x4x512 -> 8x8x256\n",
    "            nn.ConvTranspose2d(512, 256, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            # 8x8x256 -> 16x16x128\n",
    "            nn.ConvTranspose2d(256, 128, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            # 16x16x128 -> 32x32x64\n",
    "            nn.ConvTranspose2d(128, 64, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            # 32x32x64 -> 64x64x3\n",
    "            nn.ConvTranspose2d(64, 3, 4, 2, 1, bias=True),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "        # Initialize weights properly\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, (nn.Conv2d, nn.ConvTranspose2d)):\n",
    "            nn.init.normal_(module.weight, 0.0, 0.02)\n",
    "            if module.bias is not None:\n",
    "                nn.init.constant_(module.bias, 0)\n",
    "        elif isinstance(module, nn.BatchNorm2d):\n",
    "            nn.init.normal_(module.weight, 1.0, 0.02)\n",
    "            nn.init.constant_(module.bias, 0)\n",
    "        elif isinstance(module, nn.Linear):\n",
    "            nn.init.normal_(module.weight, 0.0, 0.02)\n",
    "            if module.bias is not None:\n",
    "                nn.init.constant_(module.bias, 0)\n",
    "\n",
    "    def encode(self, x):\n",
    "        # Encoder forward pass\n",
    "        features = self.encoder(x)\n",
    "        features_flat = features.view(features.size(0), -1)\n",
    "        embedding = self.fc_encode(features_flat)\n",
    "        return embedding\n",
    "\n",
    "    def decode(self, embedding):\n",
    "        # Decoder forward pass\n",
    "        features_flat = self.fc_decode(embedding)\n",
    "        features = features_flat.view(-1, 512, 2, 2)\n",
    "        reconstruction = self.decoder(features)\n",
    "        return reconstruction\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedding = self.encode(x)\n",
    "        reconstruction = self.decode(embedding)\n",
    "        return reconstruction, embedding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoencoder training on Pets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder = ImprovedAutoEncoder()\n",
    "autoencoder = autoencoder.to(device)\n",
    "\n",
    "criterion = MSELoss()\n",
    "optimizer = torch.optim.Adam(autoencoder.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "\n",
    "AE_EPOCHS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epoch_losses = []\n",
    "for j in range(AE_EPOCHS):\n",
    "\n",
    "    losses = []\n",
    "    for batch_idx, data in enumerate(tqdm(train_loader)):\n",
    "        x, _ = data\n",
    "        x = x.to(device)\n",
    "\n",
    "        # Run the autoencoder\n",
    "        _x, emb = autoencoder(x)\n",
    "        # Don't apply sigmoid since we're using tanh output and MSE loss\n",
    "\n",
    "        # Compute Reconstruction loss - both x and _x are in [-1,1] range\n",
    "        rec_loss = criterion(_x, x)\n",
    "\n",
    "        losses.append(rec_loss.item())\n",
    "\n",
    "        autoencoder.zero_grad()\n",
    "        rec_loss.backward()\n",
    "        optimizer.step()\n",
    "    epoch_losses.append(sum(losses) / len(losses))\n",
    "    print(f'Epoch #{j + 1}, Loss: {sum(losses) / len(losses):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, len(epoch_losses) + 1), epoch_losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test autoencoder reconstruction quality\n",
    "autoencoder.eval()\n",
    "\n",
    "# Get a batch of test images\n",
    "test_batch_size = 8\n",
    "test_loader = torch.utils.data.DataLoader(train_set, test_batch_size, shuffle=True)\n",
    "test_images, test_labels = next(iter(test_loader))\n",
    "test_images = test_images.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Get reconstructions\n",
    "    reconstructed, embeddings = autoencoder(test_images)\n",
    "\n",
    "\n",
    "# Convert from [-1,1] to [0,1] for display\n",
    "def denormalize(tensor):\n",
    "    return (tensor + 1) / 2\n",
    "\n",
    "\n",
    "# Create visualization\n",
    "fig, axes = plt.subplots(3, test_batch_size, figsize=(16, 6))\n",
    "\n",
    "for i in range(test_batch_size):\n",
    "    # Original image\n",
    "    orig_img = denormalize(test_images[i]).cpu().permute(1, 2, 0)\n",
    "    axes[0, i].imshow(torch.clamp(orig_img, 0, 1))\n",
    "    axes[0, i].set_title('Original')\n",
    "    axes[0, i].axis('off')\n",
    "\n",
    "    # Reconstructed image\n",
    "    recon_img = denormalize(reconstructed[i]).cpu().permute(1, 2, 0)\n",
    "    axes[1, i].imshow(torch.clamp(recon_img, 0, 1))\n",
    "    axes[1, i].set_title('Reconstructed')\n",
    "    axes[1, i].axis('off')\n",
    "\n",
    "    # Difference (error)\n",
    "    diff_img = torch.abs(orig_img - recon_img)\n",
    "    axes[2, i].imshow(diff_img, cmap='hot')\n",
    "    axes[2, i].set_title('Difference')\n",
    "    axes[2, i].axis('off')\n",
    "\n",
    "    species = 'Dog' if test_labels[i] == 1 else 'Cat'\n",
    "    axes[0, i].text(0.5, -0.1, species, transform=axes[0, i].transAxes,\n",
    "                    ha='center', fontsize=10, weight='bold')\n",
    "\n",
    "plt.suptitle('AutoEncoder Reconstruction Quality', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: the following is done to make the embeddings in a normalized scale that the NF model expects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recreate embedded dataset with normalization\n",
    "embedded_data = []\n",
    "\n",
    "# First pass: collect all embeddings to compute global statistics\n",
    "all_embeddings = []\n",
    "autoencoder.eval()\n",
    "\n",
    "for batch_idx, data in enumerate(tqdm(train_loader, desc=\"Computing embedding stats\")):\n",
    "    with torch.no_grad():\n",
    "        x, y = data\n",
    "        x = x.to(device)\n",
    "        _, emb = autoencoder(x)\n",
    "        all_embeddings.append(emb.cpu())\n",
    "\n",
    "all_embeddings = torch.cat(all_embeddings, dim=0)\n",
    "emb_mean = all_embeddings.mean(dim=0)\n",
    "emb_std = all_embeddings.std(dim=0)\n",
    "\n",
    "print(f\"Global embedding mean: {emb_mean.mean():.4f}\")\n",
    "print(f\"Global embedding std: {emb_std.mean():.4f}\")\n",
    "\n",
    "# Second pass: create normalized embedded dataset\n",
    "embedded_data = []\n",
    "for batch_idx, data in enumerate(tqdm(train_loader, desc=\"Creating normalized embeddings\")):\n",
    "    with torch.no_grad():\n",
    "        x, y = data\n",
    "        x = x.to(device)\n",
    "        _, emb = autoencoder(x)\n",
    "\n",
    "        # Normalize embeddings\n",
    "        emb_normalized = (emb.cpu() - emb_mean) / (emb_std + 1e-8)\n",
    "\n",
    "        for j in range(len(emb_normalized)):\n",
    "            embedded_data.append((emb_normalized[j], y[j]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "embedded_train_loader = torch.utils.data.DataLoader(embedded_data, BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing Flow training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **📌 RealNVP for Generating Pet Embeddings**\n",
    "Now that we have a way to represent complex pet images as 256-dimensional vectors, we can train a **Normalizing Flow** to learn the *distribution* of these vectors. We will use the same **RealNVP** model, but this time it will be *conditional*.\n",
    "\n",
    "## **🔹 Key Concepts**\n",
    "1️⃣ **Conditional Generation**: We provide the class label (0 for Cat, 1 for Dog) as an additional input to the model. This allows the flow to learn two distinct distributions within the same model and lets us control whether we generate a cat or a dog.\n",
    "\n",
    "2️⃣ **Invertible Transformation**: The model learns an invertible function `f` that maps a pet embedding `x` and its label `y` to a latent point `z` from a simple Gaussian distribution. This can be reversed to generate a new pet embedding from a random point `z` and a chosen label `y`.\n",
    "\n",
    "3️⃣ **Coupling Layers**: RealNVP uses these clever layers to split the input, transforming one part based on the other part *and the conditional label*. This makes the transformation powerful while keeping the necessary calculations efficient.\n",
    "\n",
    "\n",
    "\n",
    "## **📌 Expected Input & Output Shapes**\n",
    "- **Input (Embeddings):** `(batch_size, 256)`\n",
    "- **Conditional Input (Labels):** `(batch_size, 2)`  (One-hot encoded: [1,0] for Cat, [0,1] for Dog)\n",
    "- **Output (Latent `u`):** `(batch_size, 256)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FLOW_N = 9  # Number of affine coupling layers\n",
    "RNVP_TOPOLOGY = [200]  # Size of the hidden layers in each coupling layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nf_model = LinearRNVP(input_dim=EMBEDDING_DIM, coupling_topology=[256, 256],\n",
    "                      flow_n=12, batch_norm=True,\n",
    "                      mask_type='odds', conditioning_size=2,\n",
    "                      use_permutation=True, single_function=True).to(device)\n",
    "\n",
    "nf_optimizer = torch.optim.Adam(nf_model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "\n",
    "NF_EPOCHS = 25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Conditional Normalizing Flow\n",
    "We will now train the RealNVP model to learn the distribution of the normalized pet image embeddings, conditioned on whether the image is a cat or a dog.\n",
    "\n",
    "1️⃣ **Forward Pass** → Transform a pet embedding `emb` and its one-hot encoded label `y` into a latent vector `u`.\n",
    "\n",
    "2️⃣ **Compute Loss** → Maximize the log-likelihood of this transformation.\n",
    "\n",
    "3️⃣ **Backward Pass** → Update the flow's parameters to better model the two conditional distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for j in range(NF_EPOCHS):\n",
    "    nf_model.train()\n",
    "    losses = []\n",
    "\n",
    "    for batch_idx, data in enumerate(tqdm(embedded_train_loader)):\n",
    "        emb, y = data\n",
    "        emb = emb.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        y = nn.functional.one_hot(y, 2).to(device).float()\n",
    "\n",
    "        u, log_det = nf_model.forward(emb, y=y)\n",
    "        prior_logprob = nf_model.logprob(u)\n",
    "        log_prob = -torch.mean(prior_logprob.sum(1) + log_det)\n",
    "\n",
    "        losses.append(log_prob.item())\n",
    "\n",
    "        nf_model.zero_grad()\n",
    "        log_prob.backward()\n",
    "        nf_optimizer.step()\n",
    "\n",
    "    print(f'Epoch #{j + 1}, Loss: {sum(losses) / len(losses):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Evaluating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_n = 8\n",
    "f, axs = plt.subplots(nrows=2, ncols=sample_n, figsize=(16, 4))\n",
    "\n",
    "for ax in axs:\n",
    "    for a in ax:\n",
    "        a.set_xticklabels([])\n",
    "        a.set_yticklabels([])\n",
    "        a.set_aspect('equal')\n",
    "\n",
    "f.subplots_adjust(wspace=0, hspace=0)\n",
    "\n",
    "nf_model.eval()\n",
    "autoencoder.eval()\n",
    "with torch.no_grad():\n",
    "    species_names = ['Cat', 'Dog']\n",
    "\n",
    "    for i in range(2):\n",
    "        y = torch.nn.functional.one_hot(torch.tensor([i] * sample_n), 2).to(device).float()\n",
    "        emb, d = nf_model.sample(sample_n, y=y, return_logdet=True)\n",
    "\n",
    "        # Denormalize embeddings before passing to decoder\n",
    "        emb_denorm = emb * emb_std.to(device) + emb_mean.to(device)\n",
    "\n",
    "        z = autoencoder.decode(emb_denorm)\n",
    "\n",
    "        d_sorted = d.sort(0)[1].flip(0)\n",
    "        z = z[d_sorted]\n",
    "        z = (z + 1) / 2\n",
    "        z = torch.clamp(z, 0, 1).cpu()\n",
    "\n",
    "        for j in range(sample_n):\n",
    "            img = z[j].permute(1, 2, 0)\n",
    "            axs[i][j].imshow(img)\n",
    "            if j == 0:\n",
    "                axs[i][j].set_ylabel(species_names[i], rotation=0, labelpad=20)\n",
    "\n",
    "plt.suptitle('Generated Pet Images: Cats (top) and Dogs (bottom)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **🔹 Exercise: Exploring the Generative Landscape**\n",
    "\n",
    "The final image quality is a result of two models working in tandem. Tweaking the hyperparameters of either the Autoencoder or the Normalizing Flow can lead to better results(hopefully).\n",
    "\n",
    "### **📝 Tasks**\n",
    "\n",
    "1.  **Autoencoder Quality**: The Autoencoder was trained for 100 epochs. Try training it for longer (e.g., `AE_EPOCHS = 200`). Does a lower reconstruction loss in the AE lead to sharper, more realistic generations from the complete system?\n",
    "2.  **Embedding Dimension**: Change `EMBEDDING_DIM` to `128` (more compression) or `512` (less compression). How does this trade-off affect the detail (e.g., fur texture) and variety of the generated pets?\n",
    "3.  **Flow Complexity**: Adjust the `flow_n` (e.g., to `8` or `16`) and the `coupling_topology` in the `LinearRNVP` (e.g. `[512, 512]`). How does the flow's capacity impact its ability to model the subtle differences between cat and dog breeds?\n",
    "\n",
    "Note:\n",
    "-   A high-quality **Autoencoder is crucial**. Garbage in, garbage out; if the embeddings are poor, the Normalizing Flow cannot generate good images.\n",
    "    - This was a very big issue when I was creating the notebook 🙃"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contributed by: Ali Habibullah."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
